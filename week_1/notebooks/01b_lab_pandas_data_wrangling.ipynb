{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0084ac83",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** Â· *Intermediate AI & Data Science*\n",
    "### Week 01 Â· Lab 01B â€” Data Wrangling\n",
    "**Instructor:** Amir Charkhi  |  **Duration:** 45 minutes  |  **Difficulty:** â­â­â­â˜†â˜†\n",
    "\n",
    "> **Goal:** Master groupby, merge, pivot, and real-world data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab385f",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Master groupby operations and aggregations\n",
    "- Perform different types of merges and joins\n",
    "- Reshape data with pivot and melt\n",
    "- Handle real-world messy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a16aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for data wrangling! ðŸ”§\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Ready for data wrangling! ðŸ”§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed1ac6",
   "metadata": {},
   "source": [
    "## Part 1: GroupBy Mastery (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ecef6",
   "metadata": {},
   "source": [
    "### Exercise 1.1 â€” Sales Team Performance (medium)\n",
    "Analyze sales team performance across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bb01191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salesperson\n",
      "Diana      74760.62\n",
      "Bob        59313.60\n",
      "Charlie    53546.82\n",
      "Alice      40206.96\n",
      "Name: revenue, dtype: float64\n",
      "region\n",
      "East     2643.599333\n",
      "South    2350.263500\n",
      "North    2282.157308\n",
      "West     1757.444167\n",
      "Name: revenue, dtype: float64\n",
      "region\n",
      "East     A\n",
      "North    A\n",
      "South    C\n",
      "West     C\n",
      "Name: quantity, dtype: object\n",
      "              amount                   items                \n",
      "                 sum        mean count   sum      mean count\n",
      "customer_id                                                 \n",
      "1             745.33  248.443333     3    14  4.666667     3\n",
      "2            1304.69  326.172500     4    23  5.750000     4\n",
      "3            1246.39  311.597500     4    16  4.000000     4\n",
      "4            2214.48  276.810000     8    46  5.750000     8\n",
      "5            2716.72  301.857778     9    45  5.000000     9\n",
      "6            1159.16  386.386667     3    17  5.666667     3\n",
      "7            1070.53  267.632500     4    26  6.500000     4\n",
      "8            1452.50  242.083333     6    23  3.833333     6\n",
      "9            1606.90  229.557143     7    31  4.428571     7\n",
      "10           1711.80  244.542857     7    30  4.285714     7\n",
      "11           1126.45  225.290000     5    16  3.200000     5\n",
      "12            773.44  128.906667     6    25  4.166667     6\n",
      "13           2258.45  282.306250     8    32  4.000000     8\n",
      "14            432.45  108.112500     4    11  2.750000     4\n",
      "15           1405.89  281.178000     5    32  6.400000     5\n",
      "16           2089.40  261.175000     8    44  5.500000     8\n",
      "17            232.91  232.910000     1     9  9.000000     1\n",
      "18            916.53  305.510000     3    19  6.333333     3\n",
      "19            289.51   96.503333     3    13  4.333333     3\n",
      "20            604.93  302.465000     2     3  1.500000     2\n"
     ]
    }
   ],
   "source": [
    "# Sales data\n",
    "np.random.seed(42)\n",
    "sales = pd.DataFrame({\n",
    "    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], 100),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'product': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'quantity': np.random.randint(1, 50, 100),\n",
    "    'revenue': np.random.uniform(100, 5000, 100).round(2),\n",
    "    'date': pd.date_range('2025-01-01', periods=100)\n",
    "})\n",
    "\n",
    "# TODO: Use groupby to find:\n",
    "# 1. Total revenue by salesperson\n",
    "revenue_by_person = sales.groupby('salesperson')['revenue'].sum().sort_values(ascending=False)\n",
    "print(revenue_by_person)\n",
    "\n",
    "# 2. Average sale amount by region\n",
    "average_sales = sales.groupby('region')['revenue'].mean().sort_values(ascending=False)\n",
    "print(average_sales)\n",
    "\n",
    "# 3. Top product by quantity in each region\n",
    "top_product = sales.groupby(['region', 'product'])['quantity'].sum().groupby('region').idxmax()\n",
    "top_product_clean = top_product.apply(lambda x: x[1])  # Get second element of tuple\n",
    "print(top_product_clean)\n",
    "\n",
    "# 4. Sales performance by salesperson AND region (multi-level groupby)\n",
    "# Your code here:\n",
    "customer_summary = orders.groupby('customer_id')[['amount', 'items']].agg(['sum', 'mean', 'count'])\n",
    "print(customer_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b16f5",
   "metadata": {},
   "source": [
    "### Exercise 1.2 â€” Custom Aggregations (medium)\n",
    "Apply multiple aggregation functions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de95fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id\n",
      "17    2288.07\n",
      "4     2209.61\n",
      "5     2111.84\n",
      "12    2090.49\n",
      "16    1868.49\n",
      "Name: amount, dtype: float64\n",
      "Average order value: $273.05\n",
      "Total number of orders: 100\n",
      "Most frequent category: Clothing\n",
      "Days since last order: -7\n",
      "                 sum        mean  count\n",
      "customer_id                            \n",
      "1            1747.46  291.243333      6\n",
      "2            1009.81  252.452500      4\n",
      "3            1679.48  279.913333      6\n",
      "4            2209.61  276.201250      8\n",
      "5            2111.84  301.691429      7\n",
      "7            1113.72  278.430000      4\n",
      "8             693.39  346.695000      2\n",
      "9            1603.07  320.614000      5\n",
      "10           1186.71  237.342000      5\n",
      "11            360.46  120.153333      3\n",
      "12           2090.49  261.311250      8\n",
      "13           1094.00  218.800000      5\n",
      "14            823.82  274.606667      3\n",
      "15            670.96  335.480000      2\n",
      "16           1868.49  266.927143      7\n",
      "17           2288.07  254.230000      9\n",
      "18           1320.14  330.035000      4\n",
      "19           1797.28  256.754286      7\n",
      "20           1636.46  327.292000      5\n"
     ]
    }
   ],
   "source": [
    "# Customer orders\n",
    "orders = pd.DataFrame({\n",
    "    'customer_id': np.random.randint(1, 21, 100),\n",
    "    'order_date': pd.date_range('2025-06-01', periods=100),\n",
    "    'amount': np.random.uniform(20, 500, 100).round(2),\n",
    "    'items': np.random.randint(1, 10, 100),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], 100)\n",
    "})\n",
    "\n",
    "# TODO: Create a customer summary with:\n",
    "# 1. Total spending\n",
    "customer_summary = orders.groupby('customer_id')['amount'].sum().nlargest(5)\n",
    "print(customer_summary)\n",
    "\n",
    "# 2. Average order value\n",
    "aov = orders['amount'].mean().round(2)\n",
    "print(f\"Average order value: ${aov}\")\n",
    "\n",
    "# 3. Number of orders\n",
    "total_orders = len(orders)  # or orders.shape[0]\n",
    "print(f\"Total number of orders: {total_orders}\")\n",
    "\n",
    "# 4. Most frequent category\n",
    "frequent_category = orders['category'].value_counts().idxmax()\n",
    "print(f\"Most frequent category: {frequent_category}\")\n",
    "\n",
    "# 5. Days since last order\n",
    "most_recent_order = orders['order_date'].max()\n",
    "today = pd.Timestamp('2025-09-01')\n",
    "days_since_last = (today - most_recent_order).days\n",
    "print(f\"Days since last order: {days_since_last}\")\n",
    "\n",
    "# Use .agg() with dictionary or list of functions\n",
    "# Your code here:\n",
    "customer_summary = orders.groupby('customer_id')['amount'].agg(['sum', 'mean', 'count'])\n",
    "print(customer_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab97775",
   "metadata": {},
   "source": [
    "### Exercise 1.3 â€” Transform vs Aggregate (hard)\n",
    "Understand the difference between transform and aggregate operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ef71b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  store month  sales  average_sales\n",
      "0     A   Jan   1000    1100.000000\n",
      "1     A   Feb   1200    1100.000000\n",
      "2     A   Mar   1100    1100.000000\n",
      "3     B   Jan    800     883.333333\n",
      "4     B   Feb    900     883.333333\n",
      "5     B   Mar    950     883.333333\n",
      "6     C   Jan   1500    1550.000000\n",
      "7     C   Feb   1600    1550.000000\n",
      "8     C   Mar   1550    1550.000000\n",
      "  store month  sales  average_sales  percentage_total_sales\n",
      "0     A   Jan   1000    1100.000000               30.303030\n",
      "1     A   Feb   1200    1100.000000               36.363636\n",
      "2     A   Mar   1100    1100.000000               33.333333\n",
      "3     B   Jan    800     883.333333               30.188679\n",
      "4     B   Feb    900     883.333333               33.962264\n",
      "5     B   Mar    950     883.333333               35.849057\n",
      "6     C   Jan   1500    1550.000000               32.258065\n",
      "7     C   Feb   1600    1550.000000               34.408602\n",
      "8     C   Mar   1550    1550.000000               33.333333\n",
      "  store month  sales  average_sales  percentage_total_sales  \\\n",
      "0     A   Jan   1000    1100.000000               30.303030   \n",
      "1     A   Feb   1200    1100.000000               36.363636   \n",
      "2     A   Mar   1100    1100.000000               33.333333   \n",
      "3     B   Jan    800     883.333333               30.188679   \n",
      "4     B   Feb    900     883.333333               33.962264   \n",
      "5     B   Mar    950     883.333333               35.849057   \n",
      "6     C   Jan   1500    1550.000000               32.258065   \n",
      "7     C   Feb   1600    1550.000000               34.408602   \n",
      "8     C   Mar   1550    1550.000000               33.333333   \n",
      "\n",
      "   sales_above_store_average  \n",
      "0                      False  \n",
      "1                       True  \n",
      "2                      False  \n",
      "3                      False  \n",
      "4                       True  \n",
      "5                       True  \n",
      "6                      False  \n",
      "7                       True  \n",
      "8                      False  \n",
      "  store month  sales  average_sales  percentage_total_sales  \\\n",
      "0     A   Jan   1000    1100.000000               30.303030   \n",
      "1     A   Feb   1200    1100.000000               36.363636   \n",
      "2     A   Mar   1100    1100.000000               33.333333   \n",
      "3     B   Jan    800     883.333333               30.188679   \n",
      "4     B   Feb    900     883.333333               33.962264   \n",
      "5     B   Mar    950     883.333333               35.849057   \n",
      "6     C   Jan   1500    1550.000000               32.258065   \n",
      "7     C   Feb   1600    1550.000000               34.408602   \n",
      "8     C   Mar   1550    1550.000000               33.333333   \n",
      "\n",
      "   sales_above_store_average  months_ranking_bystore  \n",
      "0                      False                     3.0  \n",
      "1                       True                     1.0  \n",
      "2                      False                     2.0  \n",
      "3                      False                     3.0  \n",
      "4                       True                     2.0  \n",
      "5                       True                     1.0  \n",
      "6                      False                     3.0  \n",
      "7                       True                     1.0  \n",
      "8                      False                     2.0  \n"
     ]
    }
   ],
   "source": [
    "# Store sales data\n",
    "store_sales = pd.DataFrame({\n",
    "    'store': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n",
    "    'month': ['Jan', 'Feb', 'Mar'] * 3,\n",
    "    'sales': [1000, 1200, 1100, 800, 900, 950, 1500, 1600, 1550]\n",
    "})\n",
    "\n",
    "# TODO: Use groupby to:\n",
    "# 1. Add a column showing each store's average sales (use transform)\n",
    "store_sales['average_sales'] = store_sales.groupby('store')['sales'].transform('mean')\n",
    "print(store_sales)\n",
    "\n",
    "# 2. Add a column showing percentage of store's total sales\n",
    "store_sales['percentage_total_sales'] = (store_sales['sales'] / store_sales.groupby('store')['sales'].transform('sum')) * 100\n",
    "print(store_sales)\n",
    "\n",
    "# 3. Add a column indicating if sales are above store average\n",
    "store_sales['sales_above_store_average'] = store_sales['sales'] > store_sales['average_sales']\n",
    "print(store_sales)\n",
    "\n",
    "# 4. Rank months within each store by sales\n",
    "# Your code here:\n",
    "store_sales['months_ranking_bystore'] = store_sales.groupby('store')['sales'].rank(ascending=False)\n",
    "print(store_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c380357",
   "metadata": {},
   "source": [
    "## Part 2: Merging and Joining (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c66958",
   "metadata": {},
   "source": [
    "### Exercise 2.1 â€” Customer Database Integration (medium)\n",
    "Merge customer information from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f6f321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join result:\n",
      "   customer_id     name              email       city state\n",
      "0            1    Alice    alice@email.com      Perth    WA\n",
      "1            2      Bob      bob@email.com     Sydney   NSW\n",
      "2            3  Charlie  charlie@email.com  Melbourne   VIC\n",
      "Left join result:\n",
      "   customer_id     name              email       city state\n",
      "0            1    Alice    alice@email.com      Perth    WA\n",
      "1            2      Bob      bob@email.com     Sydney   NSW\n",
      "2            3  Charlie  charlie@email.com  Melbourne   VIC\n",
      "3            4    Diana    diana@email.com        NaN   NaN\n",
      "4            5      Eve      eve@email.com        NaN   NaN\n",
      "Outer join result:\n",
      "   customer_id     name              email       city state\n",
      "0            1    Alice    alice@email.com      Perth    WA\n",
      "1            2      Bob      bob@email.com     Sydney   NSW\n",
      "2            3  Charlie  charlie@email.com  Melbourne   VIC\n",
      "3            4    Diana    diana@email.com        NaN   NaN\n",
      "4            5      Eve      eve@email.com        NaN   NaN\n",
      "5            6      NaN                NaN   Brisbane   QLD\n",
      "Complete customer profile:\n",
      "   customer_id     name              email       city state  order_total\n",
      "0            1    Alice    alice@email.com      Perth    WA        150.0\n",
      "1            1    Alice    alice@email.com      Perth    WA        200.0\n",
      "2            2      Bob      bob@email.com     Sydney   NSW         75.0\n",
      "3            3  Charlie  charlie@email.com  Melbourne   VIC        300.0\n",
      "4            3  Charlie  charlie@email.com  Melbourne   VIC        125.0\n",
      "5            3  Charlie  charlie@email.com  Melbourne   VIC        180.0\n",
      "6            4    Diana    diana@email.com        NaN   NaN         90.0\n",
      "7            5      Eve      eve@email.com        NaN   NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "# Customer basic info\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com', \n",
    "             'diana@email.com', 'eve@email.com']\n",
    "})\n",
    "\n",
    "# Customer addresses\n",
    "addresses = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 6],  # Note: customer 6 doesn't exist, 4 & 5 missing\n",
    "    'city': ['Perth', 'Sydney', 'Melbourne', 'Brisbane'],\n",
    "    'state': ['WA', 'NSW', 'VIC', 'QLD']\n",
    "})\n",
    "\n",
    "# Customer orders\n",
    "customer_orders = pd.DataFrame({\n",
    "    'customer_id': [1, 1, 2, 3, 3, 3, 4],\n",
    "    'order_total': [150, 200, 75, 300, 125, 180, 90]\n",
    "})\n",
    "\n",
    "# TODO: Perform different types of merges:\n",
    "# 1. Inner join: customers with addresses\n",
    "inner_result = customers.merge(addresses, on='customer_id', how='inner')\n",
    "print(\"Inner join result:\")\n",
    "print(inner_result)\n",
    "\n",
    "# 2. Left join: all customers with their addresses (if available)\n",
    "left_result = customers.merge(addresses, on='customer_id', how='left')\n",
    "print(\"Left join result:\")\n",
    "print(left_result)\n",
    "\n",
    "# 3. Outer join: all records from both tables\n",
    "outer_result = customers.merge(addresses, on='customer_id', how='outer')\n",
    "print(\"Outer join result:\")\n",
    "print(outer_result)\n",
    "\n",
    "# 4. Merge all three tables to create complete customer profile\n",
    "# Your code here:\n",
    "complete_profile = customers.merge(addresses, on='customer_id', how='left') \\\n",
    "                           .merge(customer_orders, on='customer_id', how='left')\n",
    "print(\"Complete customer profile:\")\n",
    "print(complete_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e5dd5",
   "metadata": {},
   "source": [
    "### Exercise 2.2 â€” Product Catalog Merge (hard)\n",
    "Handle complex merges with multiple keys and conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "385a79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with actual prices:\n",
      "  product_name store_id  base_price  price_multiplier  actual_price\n",
      "0       Laptop       S1        1000              1.10       1100.00\n",
      "1       Laptop       S2        1000              0.95        950.00\n",
      "2       Laptop       S3        1000              1.20       1200.00\n",
      "3        Mouse       S1          25              1.00         25.00\n",
      "4     Keyboard       S1          75              1.05         78.75\n",
      "5      Monitor       S2         350              1.15        402.50\n",
      "Complete price list with store info:\n",
      "  store_name  location product_name  base_price  price_multiplier  \\\n",
      "0   MegaMart  Downtown       Laptop        1000              1.10   \n",
      "1  QuickShop    Suburb       Laptop        1000              0.95   \n",
      "2   TechZone      Mall       Laptop        1000              1.20   \n",
      "3   MegaMart  Downtown        Mouse          25              1.00   \n",
      "4   MegaMart  Downtown     Keyboard          75              1.05   \n",
      "5  QuickShop    Suburb      Monitor         350              1.15   \n",
      "\n",
      "   actual_price  \n",
      "0       1100.00  \n",
      "1        950.00  \n",
      "2       1200.00  \n",
      "3         25.00  \n",
      "4         78.75  \n",
      "5        402.50  \n",
      "Products not available in certain stores:\n",
      "   product_name store_name\n",
      "11     Keyboard  BudgetBuy\n",
      "3        Laptop  BudgetBuy\n",
      "15      Monitor  BudgetBuy\n",
      "7         Mouse  BudgetBuy\n",
      "12      Monitor   MegaMart\n",
      "9      Keyboard  QuickShop\n",
      "5         Mouse  QuickShop\n",
      "10     Keyboard   TechZone\n",
      "14      Monitor   TechZone\n",
      "6         Mouse   TechZone\n",
      "Price variance across stores:\n",
      "  product_id product_name     min      max     std  count  price_range\n",
      "0       P001       Laptop  950.00  1200.00  125.83      3        250.0\n",
      "1       P002        Mouse   25.00    25.00     NaN      1          0.0\n",
      "2       P003     Keyboard   78.75    78.75     NaN      1          0.0\n",
      "3       P004      Monitor  402.50   402.50     NaN      1          0.0\n"
     ]
    }
   ],
   "source": [
    "# Product catalog\n",
    "products = pd.DataFrame({\n",
    "    'product_id': ['P001', 'P002', 'P003', 'P004'],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor'],\n",
    "    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics'],\n",
    "    'base_price': [1000, 25, 75, 350]\n",
    "})\n",
    "\n",
    "# Store-specific pricing\n",
    "store_prices = pd.DataFrame({\n",
    "    'store_id': ['S1', 'S1', 'S1', 'S2', 'S2', 'S3'],\n",
    "    'product_id': ['P001', 'P002', 'P003', 'P001', 'P004', 'P001'],\n",
    "    'price_multiplier': [1.1, 1.0, 1.05, 0.95, 1.15, 1.2]\n",
    "})\n",
    "\n",
    "# Store information\n",
    "stores = pd.DataFrame({\n",
    "    'store_id': ['S1', 'S2', 'S3', 'S4'],\n",
    "    'store_name': ['MegaMart', 'QuickShop', 'TechZone', 'BudgetBuy'],\n",
    "    'location': ['Downtown', 'Suburb', 'Mall', 'Online']\n",
    "})\n",
    "\n",
    "# TODO: Create a complete price list:\n",
    "# 1. Merge to get actual prices (base_price * multiplier) for each store\n",
    "merged_data = products.merge(store_prices, on='product_id', how='inner')\n",
    "merged_data['actual_price'] = merged_data['base_price'] * merged_data['price_multiplier']\n",
    "print(\"Products with actual prices:\")\n",
    "print(merged_data[['product_name', 'store_id', 'base_price', 'price_multiplier', 'actual_price']])\n",
    "\n",
    "# 2. Include store names and locations\n",
    "complete_data = merged_data.merge(stores, on='store_id', how='left')\n",
    "print(\"Complete price list with store info:\")\n",
    "print(complete_data[['store_name', 'location', 'product_name', 'base_price', 'price_multiplier', 'actual_price']])\n",
    "\n",
    "# 3. Find products not available in certain stores\n",
    "all_combinations = products[['product_id', 'product_name']].merge(\n",
    "    stores[['store_id', 'store_name']], how='cross'\n",
    ")\n",
    "availability_check = all_combinations.merge(\n",
    "    store_prices, on=['product_id', 'store_id'], how='left', indicator=True\n",
    ")\n",
    "not_available = availability_check[availability_check['_merge'] == 'left_only']\n",
    "print(\"Products not available in certain stores:\")\n",
    "print(not_available[['product_name', 'store_name']].sort_values(['store_name', 'product_name']))\n",
    "\n",
    "# 4. Calculate price variance across stores for each product\n",
    "# Your code here:\n",
    "\n",
    "price_variance = merged_data.groupby(['product_id', 'product_name'])['actual_price'].agg([\n",
    "    'min', 'max', 'std', 'count'\n",
    "]).round(2)\n",
    "price_variance['price_range'] = price_variance['max'] - price_variance['min']\n",
    "\n",
    "price_variance = price_variance.reset_index()\n",
    "\n",
    "print(\"Price variance across stores:\")\n",
    "print(price_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7307163",
   "metadata": {},
   "source": [
    "## Part 3: Pivoting and Reshaping (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352d067",
   "metadata": {},
   "source": [
    "### Exercise 3.1 â€” Sales Matrix Creation (medium)\n",
    "Reshape data from long to wide format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cae8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long format data:\n",
      "   month region product  sales\n",
      "0    Jan  North       A   1994\n",
      "1    Jan  South       A   1220\n",
      "2    Jan   East       B   3804\n",
      "3    Jan   West       B   3014\n",
      "4    Feb  North       A   1188\n",
      "5    Feb  South       A   3265\n",
      "6    Feb   East       B   1648\n",
      "7    Feb   West       B   3946\n",
      "8    Mar  North       A   4524\n",
      "9    Mar  South       A   2674\n",
      "10   Mar   East       B   4155\n",
      "11   Mar   West       B   2402\n",
      "\n",
      "1. PIVOT: Regions as columns, months as rows\n",
      "region  East  North  South  West\n",
      "month                           \n",
      "Feb     1648   1188   3265  3946\n",
      "Jan     3804   1994   1220  3014\n",
      "Mar     4155   4524   2674  2402\n",
      "\n",
      "2. PIVOT TABLE: Product-Region sales totals\n",
      "region     East   North   South    West\n",
      "product                                \n",
      "A           NaN  7706.0  7159.0     NaN\n",
      "B        9607.0     NaN     NaN  9362.0\n",
      "\n",
      "3. PIVOT TABLE WITH MARGINS: Adding row and column totals\n",
      "   Using margins=True to add totals\n",
      "region     East   North   South    West  Total\n",
      "product                                       \n",
      "A           NaN  7706.0  7159.0     NaN  14865\n",
      "B        9607.0     NaN     NaN  9362.0  18969\n",
      "Total    9607.0  7706.0  7159.0  9362.0  33834\n",
      "\n",
      "4. MONTH-OVER-MONTH GROWTH: Calculate growth rate for each region\n",
      "   Using pct_change() to calculate percentage change\n",
      "Original monthly data by region:\n",
      "region  East  North  South  West\n",
      "month                           \n",
      "Feb     1648   1188   3265  3946\n",
      "Jan     3804   1994   1220  3014\n",
      "Mar     4155   4524   2674  2402\n",
      "\n",
      "Month-over-month growth (%):\n",
      "region    East   North   South   West\n",
      "month                                \n",
      "Feb        NaN     NaN     NaN    NaN\n",
      "Jan     130.83   67.85  -62.63 -23.62\n",
      "Mar       9.23  126.88  119.18 -20.31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monthly sales by product and region (long format)\n",
    "long_sales = pd.DataFrame({\n",
    "    'month': ['Jan', 'Jan', 'Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Feb',\n",
    "              'Mar', 'Mar', 'Mar', 'Mar'],\n",
    "    'region': ['North', 'South', 'East', 'West'] * 3,\n",
    "    'product': ['A', 'A', 'B', 'B'] * 3,\n",
    "    'sales': np.random.randint(1000, 5000, 12)\n",
    "})\n",
    "\n",
    "print(\"Long format data:\")\n",
    "print(long_sales)\n",
    "print()\n",
    "\n",
    "# TODO: Reshape the data:\n",
    "# 1. Pivot to show regions as columns, months as rows\n",
    "print(\"1. PIVOT: Regions as columns, months as rows\")\n",
    "monthly_regional_sales = long_sales.groupby(['month', 'region'])['sales'].sum().reset_index()\n",
    "pivot_regions = monthly_regional_sales.pivot(index='month', columns='region', values='sales')\n",
    "print(pivot_regions)\n",
    "print()\n",
    "\n",
    "# 2. Create a pivot table with product-region sales totals\n",
    "print(\"2. PIVOT TABLE: Product-Region sales totals\")\n",
    "pivot_table_product_region = pd.pivot_table(\n",
    "    long_sales, \n",
    "    values='sales', \n",
    "    index='product', \n",
    "    columns='region', \n",
    "    aggfunc='sum'\n",
    ")\n",
    "print(pivot_table_product_region)\n",
    "print()\n",
    "\n",
    "# 3. Add row and column totals (margins)\n",
    "print(\"3. PIVOT TABLE WITH MARGINS: Adding row and column totals\")\n",
    "print(\"   Using margins=True to add totals\")\n",
    "\n",
    "pivot_with_margins = pd.pivot_table(\n",
    "    long_sales,\n",
    "    values='sales',\n",
    "    index='product',\n",
    "    columns='region',\n",
    "    aggfunc='sum',\n",
    "    margins=True,  # This adds row and column totals\n",
    "    margins_name='Total'  # Name for the margin rows/columns\n",
    ")\n",
    "print(pivot_with_margins)\n",
    "print()\n",
    "\n",
    "# 4. Calculate month-over-month growth for each region\n",
    "# Your code here:\n",
    "print(\"4. MONTH-OVER-MONTH GROWTH: Calculate growth rate for each region\")\n",
    "print(\"   Using pct_change() to calculate percentage change\")\n",
    "\n",
    "# Start with our pivoted data (regions as columns, months as rows)\n",
    "print(\"Original monthly data by region:\")\n",
    "print(pivot_regions)\n",
    "print()\n",
    "\n",
    "# Calculate month-over-month growth (percentage change)\n",
    "mom_growth = pivot_regions.pct_change() * 100  # Convert to percentage\n",
    "\n",
    "print(\"Month-over-month growth (%):\")\n",
    "print(mom_growth.round(2))  # Round to 2 decimal places for readability\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1782d7",
   "metadata": {},
   "source": [
    "### Exercise 3.2 â€” Melt and Stack Operations (hard)\n",
    "Convert wide format data to long format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a64f54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide format grades:\n",
      "   student  Math  Science  English  History\n",
      "0    Alice    85       90       78       82\n",
      "1      Bob    78       82       85       80\n",
      "2  Charlie    92       88       80       85\n",
      "3    Diana    88       85       92       88\n",
      "\n",
      "1. MELT: Convert wide format to long format\n",
      "   Using melt() to transform columns into rows\n",
      "    student  subject  grade\n",
      "0     Alice     Math     85\n",
      "1       Bob     Math     78\n",
      "2   Charlie     Math     92\n",
      "3     Diana     Math     88\n",
      "4     Alice  Science     90\n",
      "5       Bob  Science     82\n",
      "6   Charlie  Science     88\n",
      "7     Diana  Science     85\n",
      "8     Alice  English     78\n",
      "9       Bob  English     85\n",
      "10  Charlie  English     80\n",
      "11    Diana  English     92\n",
      "12    Alice  History     82\n",
      "13      Bob  History     80\n",
      "14  Charlie  History     85\n",
      "15    Diana  History     88\n",
      "\n",
      "2. AVERAGE GRADE PER SUBJECT\n",
      "   Using groupby() to aggregate by subject\n",
      "Detailed subject statistics:\n",
      "         Average Grade  Student Count\n",
      "subject                              \n",
      "English          83.75              4\n",
      "History          83.75              4\n",
      "Math             85.75              4\n",
      "Science          86.25              4\n",
      "\n",
      "3. EACH STUDENT'S BEST AND WORST SUBJECTS\n",
      "   Using groupby() with idxmax() and idxmin()\n",
      "         Worst Grade  Best Grade  Average Grade\n",
      "student                                        \n",
      "Alice             78          90          83.75\n",
      "Bob               78          85          81.25\n",
      "Charlie           80          92          86.25\n",
      "Diana             85          92          88.25\n",
      "\n",
      "4. RANKING WITHIN EACH SUBJECT\n",
      "   Using rank() to rank students within each subject\n",
      "    student  subject  grade  rank\n",
      "11    Diana  English     92   1.0\n",
      "9       Bob  English     85   2.0\n",
      "10  Charlie  English     80   3.0\n",
      "8     Alice  English     78   4.0\n",
      "15    Diana  History     88   1.0\n",
      "14  Charlie  History     85   2.0\n",
      "12    Alice  History     82   3.0\n",
      "13      Bob  History     80   4.0\n",
      "2   Charlie     Math     92   1.0\n",
      "3     Diana     Math     88   2.0\n",
      "0     Alice     Math     85   3.0\n",
      "1       Bob     Math     78   4.0\n",
      "4     Alice  Science     90   1.0\n",
      "6   Charlie  Science     88   2.0\n",
      "7     Diana  Science     85   3.0\n",
      "5       Bob  Science     82   4.0\n",
      "\n",
      "Rankings in pivot table format (1 = best in subject):\n",
      "subject  English  History  Math  Science\n",
      "student                                 \n",
      "Alice        4.0      3.0   3.0      1.0\n",
      "Bob          2.0      4.0   4.0      4.0\n",
      "Charlie      3.0      2.0   1.0      2.0\n",
      "Diana        1.0      1.0   2.0      3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wide format grade data\n",
    "grades_wide = pd.DataFrame({\n",
    "    'student': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Math': [85, 78, 92, 88],\n",
    "    'Science': [90, 82, 88, 85],\n",
    "    'English': [78, 85, 80, 92],\n",
    "    'History': [82, 80, 85, 88]\n",
    "})\n",
    "\n",
    "print(\"Wide format grades:\")\n",
    "print(grades_wide)\n",
    "print()\n",
    "\n",
    "# TODO: Reshape the data:\n",
    "# 1. Melt to long format (student, subject, grade)\n",
    "print(\"1. MELT: Convert wide format to long format\")\n",
    "print(\"   Using melt() to transform columns into rows\")\n",
    "\n",
    "grades_long = pd.melt(grades_wide, id_vars=['student'], var_name='subject', value_name='grade')\n",
    "\n",
    "print(grades_long)\n",
    "print()\n",
    "# 2. Calculate average grade per subject\n",
    "print(\"2. AVERAGE GRADE PER SUBJECT\")\n",
    "print(\"   Using groupby() to aggregate by subject\")\n",
    "\n",
    "subject_avg_df = grades_long.groupby('subject')['grade'].agg(['mean', 'count']).round(2)\n",
    "subject_avg_df.columns = ['Average Grade', 'Student Count']\n",
    "print(\"Detailed subject statistics:\")\n",
    "print(subject_avg_df)\n",
    "print()\n",
    "\n",
    "# 3. Find each student's best and worst subjects\n",
    "print(\"3. EACH STUDENT'S BEST AND WORST SUBJECTS\")\n",
    "print(\"   Using groupby() with idxmax() and idxmin()\")\n",
    "\n",
    "student_performance = grades_long.groupby('student').agg({\n",
    "    'grade': ['min', 'max', 'mean']\n",
    "}).round(2)\n",
    "student_performance.columns = ['Worst Grade', 'Best Grade', 'Average Grade']\n",
    "print(student_performance)\n",
    "print()\n",
    "\n",
    "\n",
    "# 4. Create a ranking within each subject\n",
    "# Your code here:\n",
    "print(\"4. RANKING WITHIN EACH SUBJECT\")\n",
    "print(\"   Using rank() to rank students within each subject\")\n",
    "\n",
    "# Add ranking within each subject (1 = best grade in that subject)\n",
    "grades_with_ranking = grades_long.copy()\n",
    "grades_with_ranking['rank'] = grades_long.groupby('subject')['grade'].rank(\n",
    "    method='min',        # How to handle ties (min rank for tied values)\n",
    "    ascending=False      # False = highest grade gets rank 1\n",
    ")\n",
    "\n",
    "# Sort by subject and rank for easy viewing\n",
    "grades_ranked = grades_with_ranking.sort_values(['subject', 'rank'])\n",
    "print(grades_ranked)\n",
    "print()\n",
    "\n",
    "# Pivot table to see rankings clearly\n",
    "print(\"Rankings in pivot table format (1 = best in subject):\")\n",
    "ranking_pivot = grades_with_ranking.pivot(index='student', columns='subject', values='rank')\n",
    "print(ranking_pivot)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd0687",
   "metadata": {},
   "source": [
    "### Exercise 3.3 â€” Cross-tabulation Analysis (hard)\n",
    "Use crosstab for categorical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d86c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey Data Sample:\n",
      "  age_group product_preference satisfaction would_recommend\n",
      "0     18-25                  B          Low              No\n",
      "1     18-25                  B       Medium             Yes\n",
      "2       46+                  B          Low              No\n",
      "3     26-35                  C       Medium              No\n",
      "4     26-35                  C          Low             Yes\n",
      "5     36-45                  A          Low              No\n",
      "6     18-25                  C         High             Yes\n",
      "7     36-45                  A         High             Yes\n",
      "8     26-35                  C         High             Yes\n",
      "9     36-45                  C       Medium             Yes\n",
      "\n",
      "Dataset shape: (200, 4)\n",
      "Number of responses: 200\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1. CROSSTAB: Age Group vs Product Preference\n",
      "   Using pd.crosstab() to create frequency table\n",
      "product_preference   A   B   C  All\n",
      "age_group                          \n",
      "18-25               15  17  17   49\n",
      "26-35               15  13  12   40\n",
      "36-45               17  21  19   57\n",
      "46+                 13  21  20   54\n",
      "All                 60  72  68  200\n",
      "\n",
      "2. CROSSTAB WITH PERCENTAGES: Normalized by row\n",
      "   Using normalize='index' to get row percentages\n",
      "product_preference     A     B     C\n",
      "age_group                           \n",
      "18-25               30.6  34.7  34.7\n",
      "26-35               37.5  32.5  30.0\n",
      "36-45               29.8  36.8  33.3\n",
      "46+                 24.1  38.9  37.0\n",
      "\n",
      "3. CROSSTAB: Age Group vs Satisfaction Levels\n",
      "   Analyzing satisfaction distribution across age groups\n",
      "Raw counts:\n",
      "satisfaction  High  Low  Medium  All\n",
      "age_group                           \n",
      "18-25           16   14      19   49\n",
      "26-35           11   13      16   40\n",
      "36-45           19   22      16   57\n",
      "46+             12   22      20   54\n",
      "All             58   71      71  200\n",
      "\n",
      "Row percentages (satisfaction distribution within each age group):\n",
      "satisfaction  High   Low  Medium\n",
      "age_group                       \n",
      "18-25         32.7  28.6    38.8\n",
      "26-35         27.5  32.5    40.0\n",
      "36-45         33.3  38.6    28.1\n",
      "46+           22.2  40.7    37.0\n",
      "\n",
      "4. RECOMMENDATION RATES: By Age and Product\n",
      "   Multi-level analysis using multiple grouping variables\n",
      "Recommendation rates by age group:\n",
      "would_recommend    No   Yes\n",
      "age_group                  \n",
      "18-25            30.6  69.4\n",
      "26-35            37.5  62.5\n",
      "36-45            22.8  77.2\n",
      "46+              24.1  75.9\n",
      "\n",
      "Recommendation rates by product preference:\n",
      "would_recommend       No   Yes\n",
      "product_preference            \n",
      "A                   25.0  75.0\n",
      "B                   33.3  66.7\n",
      "C                   25.0  75.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Survey responses\n",
    "np.random.seed(50)\n",
    "survey = pd.DataFrame({\n",
    "    'age_group': np.random.choice(['18-25', '26-35', '36-45', '46+'], 200),\n",
    "    'product_preference': np.random.choice(['A', 'B', 'C'], 200),\n",
    "    'satisfaction': np.random.choice(['Low', 'Medium', 'High'], 200),\n",
    "    'would_recommend': np.random.choice(['Yes', 'No'], 200, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "print(\"Survey Data Sample:\")\n",
    "print(survey.head(10))\n",
    "print(f\"\\nDataset shape: {survey.shape}\")\n",
    "print(f\"Number of responses: {len(survey)}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# TODO: Analyze survey data:\n",
    "# 1. Create crosstab of age_group vs product_preference\n",
    "print(\"1. CROSSTAB: Age Group vs Product Preference\")\n",
    "print(\"   Using pd.crosstab() to create frequency table\")\n",
    "\n",
    "age_product_crosstab = pd.crosstab(\n",
    "    survey['age_group'], \n",
    "    survey['product_preference'],\n",
    "    margins=True  # Add row and column totals\n",
    ")\n",
    "\n",
    "print(age_product_crosstab)\n",
    "print()\n",
    "\n",
    "# 2. Add percentages (normalize by row)\n",
    "print(\"2. CROSSTAB WITH PERCENTAGES: Normalized by row\")\n",
    "print(\"   Using normalize='index' to get row percentages\")\n",
    "\n",
    "age_product_percent = pd.crosstab(\n",
    "    survey['age_group'], \n",
    "    survey['product_preference'],\n",
    "    normalize='index'  # Normalize by row (each row sums to 1)\n",
    ") * 100  # Convert to percentage\n",
    "\n",
    "print(age_product_percent.round(1))\n",
    "print()\n",
    "\n",
    "# 3. Create crosstab with satisfaction levels\n",
    "print(\"3. CROSSTAB: Age Group vs Satisfaction Levels\")\n",
    "print(\"   Analyzing satisfaction distribution across age groups\")\n",
    "\n",
    "age_satisfaction = pd.crosstab(\n",
    "    survey['age_group'],\n",
    "    survey['satisfaction'],\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "print(\"Raw counts:\")\n",
    "print(age_satisfaction)\n",
    "print()\n",
    "\n",
    "print(\"Row percentages (satisfaction distribution within each age group):\")\n",
    "age_satisfaction_percent = pd.crosstab(\n",
    "    survey['age_group'],\n",
    "    survey['satisfaction'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "print(age_satisfaction_percent.round(1))\n",
    "print()\n",
    "\n",
    "# 4. Analyze recommendation rates by age and product\n",
    "# Your code here:\n",
    "print(\"4. RECOMMENDATION RATES: By Age and Product\")\n",
    "print(\"   Multi-level analysis using multiple grouping variables\")\n",
    "\n",
    "print(\"Recommendation rates by age group:\")\n",
    "age_recommend = pd.crosstab(\n",
    "    survey['age_group'],\n",
    "    survey['would_recommend'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "print(age_recommend.round(1))\n",
    "print()\n",
    "\n",
    "print(\"Recommendation rates by product preference:\")\n",
    "product_recommend = pd.crosstab(\n",
    "    survey['product_preference'],\n",
    "    survey['would_recommend'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "print(product_recommend.round(1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8e4da",
   "metadata": {},
   "source": [
    "## Part 4: Real-World Data Cleaning (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f9bb4",
   "metadata": {},
   "source": [
    "### Exercise 4.1 â€” Messy Contact Data (hard)\n",
    "Clean real-world messy contact information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27bdcf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messy data:\n",
      "             name                 email           phone  \\\n",
      "0    John Smith    John.Smith@GMAIL.com    0412-345-678   \n",
      "1        jane doe      JANE@COMPANY.COM  (04) 9876 5432   \n",
      "2     BOB JOHNSON        bob@email..com      0401234567   \n",
      "3  Alice    Brown      alice@@email.net    04 1111 2222   \n",
      "4   charlie davis              charlie@    not provided   \n",
      "\n",
      "                       address  \n",
      "0           123 Main St, Perth  \n",
      "1                  456 Oak Ave  \n",
      "2                  Sydney, NSW  \n",
      "3                         None  \n",
      "4  789 Pine Rd, Melbourne, VIC  \n",
      "\n",
      "1. CLEANING NAMES: Proper case and removing extra spaces\n",
      "   Using str.strip(), str.title(), and regex\n",
      "Before and after name cleaning:\n",
      "         Original        Cleaned\n",
      "0    John Smith       John Smith\n",
      "1        jane doe       Jane Doe\n",
      "2     BOB JOHNSON    Bob Johnson\n",
      "3  Alice    Brown    Alice Brown\n",
      "4   charlie davis  Charlie Davis\n",
      "\n",
      "2. CLEANING EMAIL ADDRESSES: Validation and standardization\n",
      "   Using regex patterns to validate and clean emails\n",
      "Email cleaning results:\n",
      "               Original               Cleaned  Valid\n",
      "0  John.Smith@GMAIL.com  john.smith@gmail.com   True\n",
      "1      JANE@COMPANY.COM      jane@company.com   True\n",
      "2        bob@email..com         bob@email.com   True\n",
      "3      alice@@email.net       alice@email.net   True\n",
      "4              charlie@              charlie@  False\n",
      "\n",
      "3. CLEANING PHONE NUMBERS: Standardizing to single format\n",
      "   Converting to format: 04XX XXX XXX\n",
      "Phone cleaning results:\n",
      "         Original       Cleaned  Valid\n",
      "0    0412-345-678  0412 345 678   True\n",
      "1  (04) 9876 5432  0498 765 432   True\n",
      "2      0401234567  0401 234 567   True\n",
      "3    04 1111 2222  0411 112 222   True\n",
      "4    not provided          None  False\n",
      "\n",
      "Original Messy Data:\n",
      "             name                 email           phone  \\\n",
      "0    John Smith    John.Smith@GMAIL.com    0412-345-678   \n",
      "1        jane doe      JANE@COMPANY.COM  (04) 9876 5432   \n",
      "2     BOB JOHNSON        bob@email..com      0401234567   \n",
      "3  Alice    Brown      alice@@email.net    04 1111 2222   \n",
      "4   charlie davis              charlie@    not provided   \n",
      "\n",
      "                       address  \n",
      "0           123 Main St, Perth  \n",
      "1                  456 Oak Ave  \n",
      "2                  Sydney, NSW  \n",
      "3                         None  \n",
      "4  789 Pine Rd, Melbourne, VIC  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. CLEANING NAMES: Proper case and removing extra spaces\n",
      "   Using str.strip(), str.title(), and regex\n",
      "Before and after name cleaning:\n",
      "         Original        Cleaned\n",
      "0    John Smith       John Smith\n",
      "1        jane doe       Jane Doe\n",
      "2     BOB JOHNSON    Bob Johnson\n",
      "3  Alice    Brown    Alice Brown\n",
      "4   charlie davis  Charlie Davis\n",
      "\n",
      "2. CLEANING EMAIL ADDRESSES: Validation and standardization\n",
      "   Using regex patterns to validate and clean emails\n",
      "Email cleaning results:\n",
      "               Original               Cleaned  Valid\n",
      "0  John.Smith@GMAIL.com  john.smith@gmail.com   True\n",
      "1      JANE@COMPANY.COM      jane@company.com   True\n",
      "2        bob@email..com         bob@email.com   True\n",
      "3      alice@@email.net       alice@email.net   True\n",
      "4              charlie@              charlie@  False\n",
      "\n",
      "3. CLEANING PHONE NUMBERS: Standardizing to single format\n",
      "   Converting to format: 04XX XXX XXX\n",
      "Phone cleaning results:\n",
      "         Original       Cleaned  Valid\n",
      "0    0412-345-678  0412 345 678   True\n",
      "1  (04) 9876 5432  0498 765 432   True\n",
      "2      0401234567  0401 234 567   True\n",
      "3    04 1111 2222  0411 112 222   True\n",
      "4    not provided          None  False\n",
      "\n",
      "4. PARSING ADDRESSES: Extracting city and state\n",
      "   Using string operations and regex to parse address components\n",
      "Address parsing results:\n",
      "                      Original       City State  Parsed\n",
      "0           123 Main St, Perth      Perth  None    True\n",
      "1                  456 Oak Ave       None  None   False\n",
      "2                  Sydney, NSW     Sydney   NSW    True\n",
      "3                         None       None  None   False\n",
      "4  789 Pine Rd, Melbourne, VIC  Melbourne   VIC    True\n",
      "\n",
      "5. DATA QUALITY ASSESSMENT: Creating quality flags\n",
      "   Calculating completeness and validity scores\n",
      "Data Quality Assessment:\n",
      "    name_cleaned  email_valid  phone_valid  address_parsed  quality_score  \\\n",
      "0     John Smith         True         True            True            100   \n",
      "1       Jane Doe         True         True           False             75   \n",
      "2    Bob Johnson         True         True            True            100   \n",
      "3    Alice Brown         True         True           False             75   \n",
      "4  Charlie Davis        False        False            True             45   \n",
      "\n",
      "  quality_category  \n",
      "0             High  \n",
      "1           Medium  \n",
      "2             High  \n",
      "3           Medium  \n",
      "4              Low  \n",
      "\n",
      "6. FINAL CLEANED DATASET\n",
      "==================================================\n",
      "            Name                 Email         Phone       City State  \\\n",
      "0     John Smith  john.smith@gmail.com  0412 345 678      Perth  None   \n",
      "1       Jane Doe      jane@company.com  0498 765 432       None  None   \n",
      "2    Bob Johnson         bob@email.com  0401 234 567     Sydney   NSW   \n",
      "3    Alice Brown       alice@email.net  0411 112 222       None  None   \n",
      "4  Charlie Davis              charlie@          None  Melbourne   VIC   \n",
      "\n",
      "   Quality_Score Quality_Category  \n",
      "0            100             High  \n",
      "1             75           Medium  \n",
      "2            100             High  \n",
      "3             75           Medium  \n",
      "4             45              Low  \n",
      "\n",
      "7. CLEANING SUMMARY STATISTICS\n",
      "========================================\n",
      "Total records processed: 5\n",
      "Valid emails: 4/5 (80.0%)\n",
      "Valid phones: 4/5 (80.0%)\n",
      "Parsed addresses: 3/5 (60.0%)\n",
      "High quality records: 2/5 (40.0%)\n",
      "Average quality score: 79.0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Messy contact data\n",
    "contacts = pd.DataFrame({\n",
    "    'name': ['  John Smith  ', 'jane doe', 'BOB JOHNSON', 'Alice    Brown', 'charlie davis'],\n",
    "    'email': ['John.Smith@GMAIL.com', 'JANE@COMPANY.COM', 'bob@email..com', \n",
    "             'alice@@email.net', 'charlie@'],\n",
    "    'phone': ['0412-345-678', '(04) 9876 5432', '0401234567', '04 1111 2222', 'not provided'],\n",
    "    'address': ['123 Main St, Perth', '456 Oak Ave', 'Sydney, NSW', None, '789 Pine Rd, Melbourne, VIC']\n",
    "})\n",
    "\n",
    "print(\"Messy data:\")\n",
    "print(contacts)\n",
    "print()\n",
    "\n",
    "# TODO: Clean the data:\n",
    "cleaned_contacts = contacts.copy()\n",
    "\n",
    "# 1. Standardize names (proper case, remove extra spaces)\n",
    "print(\"1. CLEANING NAMES: Proper case and removing extra spaces\")\n",
    "print(\"   Using str.strip(), str.title(), and regex\")\n",
    "\n",
    "def clean_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    # Remove extra spaces and convert to title case\n",
    "    cleaned = re.sub(r'\\s+', ' ', str(name).strip())  # Replace multiple spaces with single space\n",
    "    return cleaned.title()  # Convert to proper case\n",
    "\n",
    "cleaned_contacts['name_cleaned'] = cleaned_contacts['name'].apply(clean_name)\n",
    "\n",
    "print(\"Before and after name cleaning:\")\n",
    "name_comparison = pd.DataFrame({\n",
    "    'Original': contacts['name'],\n",
    "    'Cleaned': cleaned_contacts['name_cleaned']\n",
    "})\n",
    "print(name_comparison)\n",
    "print()\n",
    "\n",
    "# 2. Validate and clean email addresses\n",
    "print(\"2. CLEANING EMAIL ADDRESSES: Validation and standardization\")\n",
    "print(\"   Using regex patterns to validate and clean emails\")\n",
    "\n",
    "def clean_email(email):\n",
    "    if pd.isna(email):\n",
    "        return email, False\n",
    "    \n",
    "    email_str = str(email).strip().lower()\n",
    "    \n",
    "    # Basic email regex pattern\n",
    "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    \n",
    "    # Fix common issues\n",
    "    # Remove double dots\n",
    "    email_str = re.sub(r'\\.+', '.', email_str)\n",
    "    # Remove double @ symbols\n",
    "    email_str = re.sub(r'@+', '@', email_str)\n",
    "    \n",
    "    # Check if email ends with @ (incomplete)\n",
    "    if email_str.endswith('@'):\n",
    "        return email_str, False\n",
    "    \n",
    "    # Validate with regex\n",
    "    is_valid = bool(re.match(email_pattern, email_str))\n",
    "    \n",
    "    return email_str, is_valid\n",
    "\n",
    "# Apply email cleaning\n",
    "email_results = cleaned_contacts['email'].apply(clean_email)\n",
    "cleaned_contacts['email_cleaned'] = [result[0] for result in email_results]\n",
    "cleaned_contacts['email_valid'] = [result[1] for result in email_results]\n",
    "\n",
    "print(\"Email cleaning results:\")\n",
    "email_comparison = pd.DataFrame({\n",
    "    'Original': contacts['email'],\n",
    "    'Cleaned': cleaned_contacts['email_cleaned'],\n",
    "    'Valid': cleaned_contacts['email_valid']\n",
    "})\n",
    "print(email_comparison)\n",
    "print()\n",
    "\n",
    "# 3. Standardize phone numbers to single format\n",
    "print(\"3. CLEANING PHONE NUMBERS: Standardizing to single format\")\n",
    "print(\"   Converting to format: 04XX XXX XXX\")\n",
    "\n",
    "def clean_phone(phone):\n",
    "    if pd.isna(phone):\n",
    "        return phone, False\n",
    "    \n",
    "    phone_str = str(phone).strip()\n",
    "    \n",
    "    # Check for non-numeric indicators\n",
    "    if 'not provided' in phone_str.lower() or phone_str.lower() == 'nan':\n",
    "        return None, False\n",
    "    \n",
    "    # Extract only digits\n",
    "    digits_only = re.sub(r'\\D', '', phone_str)\n",
    "    \n",
    "    # Australian mobile numbers should have 10 digits starting with 04\n",
    "    if len(digits_only) == 10 and digits_only.startswith('04'):\n",
    "        # Format as 04XX XXX XXX\n",
    "        formatted = f\"{digits_only[:4]} {digits_only[4:7]} {digits_only[7:]}\"\n",
    "        return formatted, True\n",
    "    else:\n",
    "        return phone_str, False\n",
    "\n",
    "# Apply phone cleaning\n",
    "phone_results = cleaned_contacts['phone'].apply(clean_phone)\n",
    "cleaned_contacts['phone_cleaned'] = [result[0] for result in phone_results]\n",
    "cleaned_contacts['phone_valid'] = [result[1] for result in phone_results]\n",
    "\n",
    "print(\"Phone cleaning results:\")\n",
    "phone_comparison = pd.DataFrame({\n",
    "    'Original': contacts['phone'],\n",
    "    'Cleaned': cleaned_contacts['phone_cleaned'],\n",
    "    'Valid': cleaned_contacts['phone_valid']\n",
    "})\n",
    "print(phone_comparison)\n",
    "print()\n",
    "\n",
    "# 4. Parse addresses to extract city and state\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Create the messy contact data\n",
    "contacts = pd.DataFrame({\n",
    "    'name': ['  John Smith  ', 'jane doe', 'BOB JOHNSON', 'Alice    Brown', 'charlie davis'],\n",
    "    'email': ['John.Smith@GMAIL.com', 'JANE@COMPANY.COM', 'bob@email..com', \n",
    "             'alice@@email.net', 'charlie@'],\n",
    "    'phone': ['0412-345-678', '(04) 9876 5432', '0401234567', '04 1111 2222', 'not provided'],\n",
    "    'address': ['123 Main St, Perth', '456 Oak Ave', 'Sydney, NSW', None, '789 Pine Rd, Melbourne, VIC']\n",
    "})\n",
    "\n",
    "print(\"Original Messy Data:\")\n",
    "print(contacts)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create a copy to work with\n",
    "cleaned_contacts = contacts.copy()\n",
    "\n",
    "# TODO 1: Standardize names (proper case, remove extra spaces)\n",
    "print(\"1. CLEANING NAMES: Proper case and removing extra spaces\")\n",
    "print(\"   Using str.strip(), str.title(), and regex\")\n",
    "\n",
    "def clean_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    # Remove extra spaces and convert to title case\n",
    "    cleaned = re.sub(r'\\s+', ' ', str(name).strip())  # Replace multiple spaces with single space\n",
    "    return cleaned.title()  # Convert to proper case\n",
    "\n",
    "cleaned_contacts['name_cleaned'] = cleaned_contacts['name'].apply(clean_name)\n",
    "\n",
    "print(\"Before and after name cleaning:\")\n",
    "name_comparison = pd.DataFrame({\n",
    "    'Original': contacts['name'],\n",
    "    'Cleaned': cleaned_contacts['name_cleaned']\n",
    "})\n",
    "print(name_comparison)\n",
    "print()\n",
    "\n",
    "# TODO 2: Validate and clean email addresses\n",
    "print(\"2. CLEANING EMAIL ADDRESSES: Validation and standardization\")\n",
    "print(\"   Using regex patterns to validate and clean emails\")\n",
    "\n",
    "def clean_email(email):\n",
    "    if pd.isna(email):\n",
    "        return email, False\n",
    "    \n",
    "    email_str = str(email).strip().lower()\n",
    "    \n",
    "    # Basic email regex pattern\n",
    "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    \n",
    "    # Fix common issues\n",
    "    # Remove double dots\n",
    "    email_str = re.sub(r'\\.+', '.', email_str)\n",
    "    # Remove double @ symbols\n",
    "    email_str = re.sub(r'@+', '@', email_str)\n",
    "    \n",
    "    # Check if email ends with @ (incomplete)\n",
    "    if email_str.endswith('@'):\n",
    "        return email_str, False\n",
    "    \n",
    "    # Validate with regex\n",
    "    is_valid = bool(re.match(email_pattern, email_str))\n",
    "    \n",
    "    return email_str, is_valid\n",
    "\n",
    "# Apply email cleaning\n",
    "email_results = cleaned_contacts['email'].apply(clean_email)\n",
    "cleaned_contacts['email_cleaned'] = [result[0] for result in email_results]\n",
    "cleaned_contacts['email_valid'] = [result[1] for result in email_results]\n",
    "\n",
    "print(\"Email cleaning results:\")\n",
    "email_comparison = pd.DataFrame({\n",
    "    'Original': contacts['email'],\n",
    "    'Cleaned': cleaned_contacts['email_cleaned'],\n",
    "    'Valid': cleaned_contacts['email_valid']\n",
    "})\n",
    "print(email_comparison)\n",
    "print()\n",
    "\n",
    "# TODO 3: Standardize phone numbers to single format\n",
    "print(\"3. CLEANING PHONE NUMBERS: Standardizing to single format\")\n",
    "print(\"   Converting to format: 04XX XXX XXX\")\n",
    "\n",
    "def clean_phone(phone):\n",
    "    if pd.isna(phone):\n",
    "        return phone, False\n",
    "    \n",
    "    phone_str = str(phone).strip()\n",
    "    \n",
    "    # Check for non-numeric indicators\n",
    "    if 'not provided' in phone_str.lower() or phone_str.lower() == 'nan':\n",
    "        return None, False\n",
    "    \n",
    "    # Extract only digits\n",
    "    digits_only = re.sub(r'\\D', '', phone_str)\n",
    "    \n",
    "    # Australian mobile numbers should have 10 digits starting with 04\n",
    "    if len(digits_only) == 10 and digits_only.startswith('04'):\n",
    "        # Format as 04XX XXX XXX\n",
    "        formatted = f\"{digits_only[:4]} {digits_only[4:7]} {digits_only[7:]}\"\n",
    "        return formatted, True\n",
    "    else:\n",
    "        return phone_str, False\n",
    "\n",
    "# Apply phone cleaning\n",
    "phone_results = cleaned_contacts['phone'].apply(clean_phone)\n",
    "cleaned_contacts['phone_cleaned'] = [result[0] for result in phone_results]\n",
    "cleaned_contacts['phone_valid'] = [result[1] for result in phone_results]\n",
    "\n",
    "print(\"Phone cleaning results:\")\n",
    "phone_comparison = pd.DataFrame({\n",
    "    'Original': contacts['phone'],\n",
    "    'Cleaned': cleaned_contacts['phone_cleaned'],\n",
    "    'Valid': cleaned_contacts['phone_valid']\n",
    "})\n",
    "print(phone_comparison)\n",
    "print()\n",
    "\n",
    "# TODO 4: Parse addresses to extract city and state\n",
    "print(\"4. PARSING ADDRESSES: Extracting city and state\")\n",
    "print(\"   Using string operations and regex to parse address components\")\n",
    "\n",
    "def parse_address(address):\n",
    "    if pd.isna(address):\n",
    "        return None, None, False\n",
    "    \n",
    "    address_str = str(address).strip()\n",
    "    \n",
    "    # Australian states (common abbreviations)\n",
    "    aus_states = ['NSW', 'VIC', 'QLD', 'SA', 'WA', 'TAS', 'NT', 'ACT']\n",
    "    \n",
    "    # Street indicators that suggest this is a street address, not a city\n",
    "    street_indicators = ['st', 'street', 'ave', 'avenue', 'rd', 'road', 'dr', 'drive', \n",
    "                        'ln', 'lane', 'cres', 'crescent', 'pl', 'place', 'ct', 'court']\n",
    "    \n",
    "    # Split by commas\n",
    "    parts = [part.strip() for part in address_str.split(',')]\n",
    "    \n",
    "    city = None\n",
    "    state = None\n",
    "    \n",
    "    if len(parts) >= 2:\n",
    "        # Look for state in the last part\n",
    "        last_part = parts[-1].upper()\n",
    "        state_found = False\n",
    "        \n",
    "        for aus_state in aus_states:\n",
    "            if aus_state in last_part:\n",
    "                state = aus_state\n",
    "                state_found = True\n",
    "                # If state found, city is the second-to-last part\n",
    "                if len(parts) >= 2:\n",
    "                    city = parts[-2].title()\n",
    "                break\n",
    "        \n",
    "        # If no state found, check if last part looks like a city (not a street address)\n",
    "        if not state_found:\n",
    "            last_part_lower = parts[-1].lower()\n",
    "            # Only treat as city if it doesn't contain street indicators\n",
    "            contains_street_indicator = any(indicator in last_part_lower for indicator in street_indicators)\n",
    "            if not contains_street_indicator:\n",
    "                city = parts[-1].title()\n",
    "            \n",
    "    elif len(parts) == 1:\n",
    "        # Single part - check if it contains state\n",
    "        single_part = parts[0]\n",
    "        state_found = False\n",
    "        \n",
    "        for aus_state in aus_states:\n",
    "            if aus_state in single_part.upper():\n",
    "                state = aus_state\n",
    "                # Extract city by removing state\n",
    "                city_part = single_part.upper().replace(aus_state, '').replace(',', '').strip()\n",
    "                if city_part:\n",
    "                    city = city_part.title()\n",
    "                state_found = True\n",
    "                break\n",
    "        \n",
    "        # If no state found, check if it looks like a city (not a street address)\n",
    "        if not state_found:\n",
    "            single_part_lower = single_part.lower()\n",
    "            contains_street_indicator = any(indicator in single_part_lower for indicator in street_indicators)\n",
    "            # Only treat as city if it doesn't contain street indicators AND doesn't start with numbers\n",
    "            if not contains_street_indicator and not re.match(r'^\\d+', single_part.strip()):\n",
    "                city = single_part.title()\n",
    "    \n",
    "    # Determine if parsing was successful (only if we found meaningful city/state info)\n",
    "    parsed_successfully = bool(city or state)\n",
    "    \n",
    "    return city, state, parsed_successfully\n",
    "\n",
    "# Apply address parsing\n",
    "address_results = cleaned_contacts['address'].apply(parse_address)\n",
    "cleaned_contacts['city'] = [result[0] for result in address_results]\n",
    "cleaned_contacts['state'] = [result[1] for result in address_results]\n",
    "cleaned_contacts['address_parsed'] = [result[2] for result in address_results]\n",
    "\n",
    "print(\"Address parsing results:\")\n",
    "address_comparison = pd.DataFrame({\n",
    "    'Original': contacts['address'],\n",
    "    'City': cleaned_contacts['city'],\n",
    "    'State': cleaned_contacts['state'],\n",
    "    'Parsed': cleaned_contacts['address_parsed']\n",
    "})\n",
    "print(address_comparison)\n",
    "print()\n",
    "\n",
    "# 5. Create data quality flags for each record\n",
    "# Your code here:\n",
    "print(\"5. DATA QUALITY ASSESSMENT: Creating quality flags\")\n",
    "print(\"   Calculating completeness and validity scores\")\n",
    "\n",
    "# Calculate quality metrics for each record\n",
    "cleaned_contacts['name_complete'] = cleaned_contacts['name_cleaned'].notna()\n",
    "cleaned_contacts['email_complete'] = cleaned_contacts['email_cleaned'].notna()\n",
    "cleaned_contacts['phone_complete'] = cleaned_contacts['phone_cleaned'].notna()\n",
    "cleaned_contacts['address_complete'] = cleaned_contacts['address'].notna()\n",
    "\n",
    "# Calculate overall quality score (0-100)\n",
    "quality_weights = {\n",
    "    'name_complete': 20,\n",
    "    'email_valid': 30,\n",
    "    'phone_valid': 25,\n",
    "    'address_parsed': 25\n",
    "}\n",
    "\n",
    "def calculate_quality_score(row):\n",
    "    score = 0\n",
    "    if row['name_complete']:\n",
    "        score += quality_weights['name_complete']\n",
    "    if row['email_valid']:\n",
    "        score += quality_weights['email_valid']\n",
    "    if row['phone_valid']:\n",
    "        score += quality_weights['phone_valid']\n",
    "    if row['address_parsed']:\n",
    "        score += quality_weights['address_parsed']\n",
    "    return score\n",
    "\n",
    "cleaned_contacts['quality_score'] = cleaned_contacts.apply(calculate_quality_score, axis=1)\n",
    "\n",
    "# Create quality categories\n",
    "def categorize_quality(score):\n",
    "    if score >= 80:\n",
    "        return 'High'\n",
    "    elif score >= 60:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "cleaned_contacts['quality_category'] = cleaned_contacts['quality_score'].apply(categorize_quality)\n",
    "\n",
    "print(\"Data Quality Assessment:\")\n",
    "quality_summary = cleaned_contacts[[\n",
    "    'name_cleaned', 'email_valid', 'phone_valid', 'address_parsed', \n",
    "    'quality_score', 'quality_category'\n",
    "]].copy()\n",
    "\n",
    "print(quality_summary)\n",
    "print()\n",
    "\n",
    "# Final cleaned dataset\n",
    "print(\"6. FINAL CLEANED DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_dataset = cleaned_contacts[[\n",
    "    'name_cleaned', 'email_cleaned', 'phone_cleaned', \n",
    "    'city', 'state', 'quality_score', 'quality_category'\n",
    "]].copy()\n",
    "\n",
    "final_dataset.columns = ['Name', 'Email', 'Phone', 'City', 'State', 'Quality_Score', 'Quality_Category']\n",
    "print(final_dataset)\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"7. CLEANING SUMMARY STATISTICS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "total_records = len(contacts)\n",
    "valid_emails = cleaned_contacts['email_valid'].sum()\n",
    "valid_phones = cleaned_contacts['phone_valid'].sum()\n",
    "parsed_addresses = cleaned_contacts['address_parsed'].sum()\n",
    "high_quality = (cleaned_contacts['quality_category'] == 'High').sum()\n",
    "\n",
    "print(f\"Total records processed: {total_records}\")\n",
    "print(f\"Valid emails: {valid_emails}/{total_records} ({valid_emails/total_records*100:.1f}%)\")\n",
    "print(f\"Valid phones: {valid_phones}/{total_records} ({valid_phones/total_records*100:.1f}%)\")\n",
    "print(f\"Parsed addresses: {parsed_addresses}/{total_records} ({parsed_addresses/total_records*100:.1f}%)\")\n",
    "print(f\"High quality records: {high_quality}/{total_records} ({high_quality/total_records*100:.1f}%)\")\n",
    "print(f\"Average quality score: {cleaned_contacts['quality_score'].mean():.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678eb045",
   "metadata": {},
   "source": [
    "### Exercise 4.2 â€” Duplicate Detection and Resolution (hard)\n",
    "Find and handle duplicate records intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f99187e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer records:\n",
      "   customer_id            name                  email last_purchase  \\\n",
      "0            1      John Smith         john@email.com    2025-01-15   \n",
      "1            2        J. Smith      j.smith@email.com    2025-02-20   \n",
      "2            3        Jane Doe         jane@gmail.com    2025-03-10   \n",
      "3            4     Jane M. Doe     jane.doe@gmail.com    2025-03-12   \n",
      "4            5     Bob Johnson          bob@email.com    2025-01-20   \n",
      "5            6  Robert Johnson          bob@email.com    2025-02-15   \n",
      "6            7     Alice Brown        alice@email.com    2025-03-01   \n",
      "7            8        Alice B.  alice.brown@email.com    2025-03-05   \n",
      "\n",
      "   total_spent  \n",
      "0          500  \n",
      "1          750  \n",
      "2         1000  \n",
      "3          200  \n",
      "4          300  \n",
      "5          450  \n",
      "6          800  \n",
      "7          150  \n",
      "\n",
      "1. EXACT EMAIL DUPLICATES\n",
      "   Finding records with identical email addresses\n",
      "Records with duplicate emails:\n",
      "   customer_id            name          email last_purchase  total_spent\n",
      "4            5     Bob Johnson  bob@email.com    2025-01-20          300\n",
      "5            6  Robert Johnson  bob@email.com    2025-02-15          450\n",
      "\n",
      "Duplicate email groups:\n",
      "\n",
      "Email: bob@email.com\n",
      "   customer_id            name last_purchase  total_spent\n",
      "4            5     Bob Johnson    2025-01-20          300\n",
      "5            6  Robert Johnson    2025-02-15          450\n",
      "\n",
      "2. SIMILAR NAME DUPLICATES\n",
      "   Using string similarity to find potential name matches\n",
      "Potential name duplicates (similarity >= 0.6):\n",
      "   customer_id_1       name_1  customer_id_2          name_2  similarity\n",
      "0              1   John Smith              2        J. Smith       0.778\n",
      "1              3     Jane Doe              4     Jane M. Doe       0.842\n",
      "2              5  Bob Johnson              6  Robert Johnson       0.800\n",
      "3              7  Alice Brown              8        Alice B.       0.737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "# Customer records with potential duplicates\n",
    "customers_dup = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'name': ['John Smith', 'J. Smith', 'Jane Doe', 'Jane M. Doe', \n",
    "            'Bob Johnson', 'Robert Johnson', 'Alice Brown', 'Alice B.'],\n",
    "    'email': ['john@email.com', 'j.smith@email.com', 'jane@gmail.com', 'jane.doe@gmail.com',\n",
    "             'bob@email.com', 'bob@email.com', 'alice@email.com', 'alice.brown@email.com'],\n",
    "    'last_purchase': pd.to_datetime(['2025-01-15', '2025-02-20', '2025-03-10', '2025-03-12',\n",
    "                                     '2025-01-20', '2025-02-15', '2025-03-01', '2025-03-05']),\n",
    "    'total_spent': [500, 750, 1000, 200, 300, 450, 800, 150]\n",
    "})\n",
    "\n",
    "print(\"Customer records:\")\n",
    "print(customers_dup)\n",
    "print()\n",
    "\n",
    "# TODO: Handle duplicates:\n",
    "# 1. Find exact email duplicates\n",
    "print(\"1. EXACT EMAIL DUPLICATES\")\n",
    "print(\"   Finding records with identical email addresses\")\n",
    "\n",
    "# Find duplicated emails\n",
    "email_duplicates = customers_dup[customers_dup.duplicated(subset=['email'], keep=False)]\n",
    "email_duplicates_sorted = email_duplicates.sort_values('email')\n",
    "\n",
    "print(\"Records with duplicate emails:\")\n",
    "print(email_duplicates_sorted)\n",
    "print()\n",
    "\n",
    "# Group by email to see duplicates clearly\n",
    "print(\"Duplicate email groups:\")\n",
    "for email, group in customers_dup.groupby('email'):\n",
    "    if len(group) > 1:\n",
    "        print(f\"\\nEmail: {email}\")\n",
    "        print(group[['customer_id', 'name', 'last_purchase', 'total_spent']])\n",
    "print()\n",
    "\n",
    "\n",
    "# 2. Find potential name duplicates (similar names)\n",
    "print(\"2. SIMILAR NAME DUPLICATES\")\n",
    "print(\"   Using string similarity to find potential name matches\")\n",
    "\n",
    "def name_similarity(name1, name2):\n",
    "    \"\"\"Calculate similarity between two names using SequenceMatcher\"\"\"\n",
    "    # Normalize names (lowercase, remove extra spaces)\n",
    "    name1_clean = re.sub(r'\\s+', ' ', str(name1).lower().strip())\n",
    "    name2_clean = re.sub(r'\\s+', ' ', str(name2).lower().strip())\n",
    "    \n",
    "    # Calculate similarity ratio\n",
    "    similarity = SequenceMatcher(None, name1_clean, name2_clean).ratio()\n",
    "    return similarity\n",
    "\n",
    "def find_name_duplicates(df, threshold=0.7):\n",
    "    \"\"\"Find potential name duplicates based on similarity threshold\"\"\"\n",
    "    potential_duplicates = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            name1 = df.iloc[i]['name']\n",
    "            name2 = df.iloc[j]['name']\n",
    "            similarity = name_similarity(name1, name2)\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                potential_duplicates.append({\n",
    "                    'customer_id_1': df.iloc[i]['customer_id'],\n",
    "                    'name_1': name1,\n",
    "                    'customer_id_2': df.iloc[j]['customer_id'], \n",
    "                    'name_2': name2,\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(potential_duplicates)\n",
    "\n",
    "# Find similar names\n",
    "name_duplicates = find_name_duplicates(customers_dup, threshold=0.6)\n",
    "print(\"Potential name duplicates (similarity >= 0.6):\")\n",
    "print(name_duplicates.round(3))\n",
    "print()\n",
    "\n",
    "\n",
    "# 3. Merge duplicate records (keep most recent, sum totals)\n",
    "# 4. Create a deduplication report\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94eef7",
   "metadata": {},
   "source": [
    "## ðŸš€ Challenge: Complete Data Pipeline\n",
    "Build an end-to-end data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-commerce data pipeline challenge\n",
    "# You have three data sources that need to be combined and analyzed\n",
    "\n",
    "# Source 1: Order data\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': range(1, 101),\n",
    "    'customer_id': np.random.randint(1, 31, 100),\n",
    "    'product_id': np.random.choice(['P1', 'P2', 'P3', 'P4', 'P5'], 100),\n",
    "    'quantity': np.random.randint(1, 5, 100),\n",
    "    'order_date': pd.date_range('2025-07-01', periods=100),\n",
    "    'status': np.random.choice(['Completed', 'Pending', 'Cancelled'], 100, p=[0.8, 0.15, 0.05])\n",
    "})\n",
    "\n",
    "# Source 2: Product data\n",
    "products = pd.DataFrame({\n",
    "    'product_id': ['P1', 'P2', 'P3', 'P4', 'P5'],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'],\n",
    "    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'],\n",
    "    'unit_price': [1200, 25, 80, 350, 120],\n",
    "    'cost': [800, 15, 50, 250, 70]\n",
    "})\n",
    "\n",
    "# Source 3: Customer data\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1, 31),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1, 31)],\n",
    "    'segment': np.random.choice(['Premium', 'Standard', 'Basic'], 30, p=[0.2, 0.5, 0.3]),\n",
    "    'join_date': pd.date_range('2024-01-01', periods=30, freq='W')\n",
    "})\n",
    "\n",
    "# TODO: Build a complete analysis pipeline:\n",
    "# 1. Merge all three datasets\n",
    "# 2. Calculate order values and profit margins\n",
    "# 3. Analyze sales by customer segment and product category\n",
    "# 4. Find top customers and products\n",
    "# 5. Calculate customer lifetime value\n",
    "# 6. Create monthly sales trend\n",
    "# 7. Identify cross-selling opportunities\n",
    "# 8. Generate executive summary DataFrame\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f420e9",
   "metadata": {},
   "source": [
    "## ðŸ“Š Lab Summary Checklist\n",
    "\n",
    "**Core Skills Practiced:**\n",
    "- [ ] GroupBy with single and multiple columns\n",
    "- [ ] Custom aggregations with agg()\n",
    "- [ ] Different types of merges (inner, left, outer)\n",
    "- [ ] Pivot tables and reshaping\n",
    "- [ ] Data cleaning and deduplication\n",
    "- [ ] Complete data pipeline\n",
    "\n",
    "**Self-Assessment:**\n",
    "- I can group and aggregate data efficiently âœ…\n",
    "- I understand different join types âœ…\n",
    "- I can reshape data between wide and long formats âœ…\n",
    "- I can clean messy real-world data âœ…\n",
    "- I can build data processing pipelines âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75456f1a",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What's Next?\n",
    "**Lab 01C:** Advanced EDA techniques and statistical analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv]",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
