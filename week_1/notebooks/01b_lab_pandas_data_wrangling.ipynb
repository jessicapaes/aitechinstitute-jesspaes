{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0084ac83",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** Â· *Intermediate AI & Data Science*\n",
    "### Week 01 Â· Lab 01B â€” Data Wrangling\n",
    "**Instructor:** Amir Charkhi  |  **Duration:** 45 minutes  |  **Difficulty:** â­â­â­â˜†â˜†\n",
    "\n",
    "> **Goal:** Master groupby, merge, pivot, and real-world data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab385f",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Master groupby operations and aggregations\n",
    "- Perform different types of merges and joins\n",
    "- Reshape data with pivot and melt\n",
    "- Handle real-world messy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Ready for data wrangling! ðŸ”§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed1ac6",
   "metadata": {},
   "source": [
    "## Part 1: GroupBy Mastery (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ecef6",
   "metadata": {},
   "source": [
    "### Exercise 1.1 â€” Sales Team Performance (medium)\n",
    "Analyze sales team performance across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb01191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales data\n",
    "np.random.seed(42)\n",
    "sales = pd.DataFrame({\n",
    "    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], 100),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'product': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'quantity': np.random.randint(1, 50, 100),\n",
    "    'revenue': np.random.uniform(100, 5000, 100).round(2),\n",
    "    'date': pd.date_range('2025-01-01', periods=100)\n",
    "})\n",
    "\n",
    "# TODO: Use groupby to find:\n",
    "# 1. Total revenue by salesperson\n",
    "# 2. Average sale amount by region\n",
    "# 3. Top product by quantity in each region\n",
    "# 4. Sales performance by salesperson AND region (multi-level groupby)\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b16f5",
   "metadata": {},
   "source": [
    "### Exercise 1.2 â€” Custom Aggregations (medium)\n",
    "Apply multiple aggregation functions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer orders\n",
    "orders = pd.DataFrame({\n",
    "    'customer_id': np.random.randint(1, 21, 100),\n",
    "    'order_date': pd.date_range('2025-06-01', periods=100),\n",
    "    'amount': np.random.uniform(20, 500, 100).round(2),\n",
    "    'items': np.random.randint(1, 10, 100),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], 100)\n",
    "})\n",
    "\n",
    "# TODO: Create a customer summary with:\n",
    "# 1. Total spending\n",
    "# 2. Average order value\n",
    "# 3. Number of orders\n",
    "# 4. Most frequent category\n",
    "# 5. Days since last order\n",
    "# Use .agg() with dictionary or list of functions\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab97775",
   "metadata": {},
   "source": [
    "### Exercise 1.3 â€” Transform vs Aggregate (hard)\n",
    "Understand the difference between transform and aggregate operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef71b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store sales data\n",
    "store_sales = pd.DataFrame({\n",
    "    'store': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n",
    "    'month': ['Jan', 'Feb', 'Mar'] * 3,\n",
    "    'sales': [1000, 1200, 1100, 800, 900, 950, 1500, 1600, 1550]\n",
    "})\n",
    "\n",
    "# TODO: Use groupby to:\n",
    "# 1. Add a column showing each store's average sales (use transform)\n",
    "# 2. Add a column showing percentage of store's total sales\n",
    "# 3. Add a column indicating if sales are above store average\n",
    "# 4. Rank months within each store by sales\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c380357",
   "metadata": {},
   "source": [
    "## Part 2: Merging and Joining (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c66958",
   "metadata": {},
   "source": [
    "### Exercise 2.1 â€” Customer Database Integration (medium)\n",
    "Merge customer information from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer basic info\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com', \n",
    "             'diana@email.com', 'eve@email.com']\n",
    "})\n",
    "\n",
    "# Customer addresses\n",
    "addresses = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 6],  # Note: customer 6 doesn't exist, 4 & 5 missing\n",
    "    'city': ['Perth', 'Sydney', 'Melbourne', 'Brisbane'],\n",
    "    'state': ['WA', 'NSW', 'VIC', 'QLD']\n",
    "})\n",
    "\n",
    "# Customer orders\n",
    "customer_orders = pd.DataFrame({\n",
    "    'customer_id': [1, 1, 2, 3, 3, 3, 4],\n",
    "    'order_total': [150, 200, 75, 300, 125, 180, 90]\n",
    "})\n",
    "\n",
    "# TODO: Perform different types of merges:\n",
    "# 1. Inner join: customers with addresses\n",
    "# 2. Left join: all customers with their addresses (if available)\n",
    "# 3. Outer join: all records from both tables\n",
    "# 4. Merge all three tables to create complete customer profile\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e5dd5",
   "metadata": {},
   "source": [
    "### Exercise 2.2 â€” Product Catalog Merge (hard)\n",
    "Handle complex merges with multiple keys and conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product catalog\n",
    "products = pd.DataFrame({\n",
    "    'product_id': ['P001', 'P002', 'P003', 'P004'],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor'],\n",
    "    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics'],\n",
    "    'base_price': [1000, 25, 75, 350]\n",
    "})\n",
    "\n",
    "# Store-specific pricing\n",
    "store_prices = pd.DataFrame({\n",
    "    'store_id': ['S1', 'S1', 'S1', 'S2', 'S2', 'S3'],\n",
    "    'product_id': ['P001', 'P002', 'P003', 'P001', 'P004', 'P001'],\n",
    "    'price_multiplier': [1.1, 1.0, 1.05, 0.95, 1.15, 1.2]\n",
    "})\n",
    "\n",
    "# Store information\n",
    "stores = pd.DataFrame({\n",
    "    'store_id': ['S1', 'S2', 'S3', 'S4'],\n",
    "    'store_name': ['MegaMart', 'QuickShop', 'TechZone', 'BudgetBuy'],\n",
    "    'location': ['Downtown', 'Suburb', 'Mall', 'Online']\n",
    "})\n",
    "\n",
    "# TODO: Create a complete price list:\n",
    "# 1. Merge to get actual prices (base_price * multiplier) for each store\n",
    "# 2. Include store names and locations\n",
    "# 3. Find products not available in certain stores\n",
    "# 4. Calculate price variance across stores for each product\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7307163",
   "metadata": {},
   "source": [
    "## Part 3: Pivoting and Reshaping (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352d067",
   "metadata": {},
   "source": [
    "### Exercise 3.1 â€” Sales Matrix Creation (medium)\n",
    "Reshape data from long to wide format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cae8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales by product and region (long format)\n",
    "long_sales = pd.DataFrame({\n",
    "    'month': ['Jan', 'Jan', 'Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Feb',\n",
    "              'Mar', 'Mar', 'Mar', 'Mar'],\n",
    "    'region': ['North', 'South', 'East', 'West'] * 3,\n",
    "    'product': ['A', 'A', 'B', 'B'] * 3,\n",
    "    'sales': np.random.randint(1000, 5000, 12)\n",
    "})\n",
    "\n",
    "print(\"Long format data:\")\n",
    "print(long_sales)\n",
    "print()\n",
    "\n",
    "# TODO: Reshape the data:\n",
    "# 1. Pivot to show regions as columns, months as rows\n",
    "# 2. Create a pivot table with product-region sales totals\n",
    "# 3. Add row and column totals (margins)\n",
    "# 4. Calculate month-over-month growth for each region\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1782d7",
   "metadata": {},
   "source": [
    "### Exercise 3.2 â€” Melt and Stack Operations (hard)\n",
    "Convert wide format data to long format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide format grade data\n",
    "grades_wide = pd.DataFrame({\n",
    "    'student': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Math': [85, 78, 92, 88],\n",
    "    'Science': [90, 82, 88, 85],\n",
    "    'English': [78, 85, 80, 92],\n",
    "    'History': [82, 80, 85, 88]\n",
    "})\n",
    "\n",
    "print(\"Wide format grades:\")\n",
    "print(grades_wide)\n",
    "print()\n",
    "\n",
    "# TODO: Reshape the data:\n",
    "# 1. Melt to long format (student, subject, grade)\n",
    "# 2. Calculate average grade per subject\n",
    "# 3. Find each student's best and worst subjects\n",
    "# 4. Create a ranking within each subject\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd0687",
   "metadata": {},
   "source": [
    "### Exercise 3.3 â€” Cross-tabulation Analysis (hard)\n",
    "Use crosstab for categorical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d86c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey responses\n",
    "np.random.seed(50)\n",
    "survey = pd.DataFrame({\n",
    "    'age_group': np.random.choice(['18-25', '26-35', '36-45', '46+'], 200),\n",
    "    'product_preference': np.random.choice(['A', 'B', 'C'], 200),\n",
    "    'satisfaction': np.random.choice(['Low', 'Medium', 'High'], 200),\n",
    "    'would_recommend': np.random.choice(['Yes', 'No'], 200, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "# TODO: Analyze survey data:\n",
    "# 1. Create crosstab of age_group vs product_preference\n",
    "# 2. Add percentages (normalize by row)\n",
    "# 3. Create crosstab with satisfaction levels\n",
    "# 4. Analyze recommendation rates by age and product\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8e4da",
   "metadata": {},
   "source": [
    "## Part 4: Real-World Data Cleaning (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f9bb4",
   "metadata": {},
   "source": [
    "### Exercise 4.1 â€” Messy Contact Data (hard)\n",
    "Clean real-world messy contact information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messy contact data\n",
    "contacts = pd.DataFrame({\n",
    "    'name': ['  John Smith  ', 'jane doe', 'BOB JOHNSON', 'Alice    Brown', 'charlie davis'],\n",
    "    'email': ['John.Smith@GMAIL.com', 'JANE@COMPANY.COM', 'bob@email..com', \n",
    "             'alice@@email.net', 'charlie@'],\n",
    "    'phone': ['0412-345-678', '(04) 9876 5432', '0401234567', '04 1111 2222', 'not provided'],\n",
    "    'address': ['123 Main St, Perth', '456 Oak Ave', 'Sydney, NSW', None, '789 Pine Rd, Melbourne, VIC']\n",
    "})\n",
    "\n",
    "print(\"Messy data:\")\n",
    "print(contacts)\n",
    "print()\n",
    "\n",
    "# TODO: Clean the data:\n",
    "# 1. Standardize names (proper case, remove extra spaces)\n",
    "# 2. Validate and clean email addresses\n",
    "# 3. Standardize phone numbers to single format\n",
    "# 4. Parse addresses to extract city and state\n",
    "# 5. Create data quality flags for each record\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678eb045",
   "metadata": {},
   "source": [
    "### Exercise 4.2 â€” Duplicate Detection and Resolution (hard)\n",
    "Find and handle duplicate records intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99187e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer records with potential duplicates\n",
    "customers_dup = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'name': ['John Smith', 'J. Smith', 'Jane Doe', 'Jane M. Doe', \n",
    "            'Bob Johnson', 'Robert Johnson', 'Alice Brown', 'Alice B.'],\n",
    "    'email': ['john@email.com', 'j.smith@email.com', 'jane@gmail.com', 'jane.doe@gmail.com',\n",
    "             'bob@email.com', 'bob@email.com', 'alice@email.com', 'alice.brown@email.com'],\n",
    "    'last_purchase': pd.to_datetime(['2025-01-15', '2025-02-20', '2025-03-10', '2025-03-12',\n",
    "                                     '2025-01-20', '2025-02-15', '2025-03-01', '2025-03-05']),\n",
    "    'total_spent': [500, 750, 1000, 200, 300, 450, 800, 150]\n",
    "})\n",
    "\n",
    "print(\"Customer records:\")\n",
    "print(customers_dup)\n",
    "print()\n",
    "\n",
    "# TODO: Handle duplicates:\n",
    "# 1. Find exact email duplicates\n",
    "# 2. Find potential name duplicates (similar names)\n",
    "# 3. Merge duplicate records (keep most recent, sum totals)\n",
    "# 4. Create a deduplication report\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94eef7",
   "metadata": {},
   "source": [
    "## ðŸš€ Challenge: Complete Data Pipeline\n",
    "Build an end-to-end data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-commerce data pipeline challenge\n",
    "# You have three data sources that need to be combined and analyzed\n",
    "\n",
    "# Source 1: Order data\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': range(1, 101),\n",
    "    'customer_id': np.random.randint(1, 31, 100),\n",
    "    'product_id': np.random.choice(['P1', 'P2', 'P3', 'P4', 'P5'], 100),\n",
    "    'quantity': np.random.randint(1, 5, 100),\n",
    "    'order_date': pd.date_range('2025-07-01', periods=100),\n",
    "    'status': np.random.choice(['Completed', 'Pending', 'Cancelled'], 100, p=[0.8, 0.15, 0.05])\n",
    "})\n",
    "\n",
    "# Source 2: Product data\n",
    "products = pd.DataFrame({\n",
    "    'product_id': ['P1', 'P2', 'P3', 'P4', 'P5'],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'],\n",
    "    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'],\n",
    "    'unit_price': [1200, 25, 80, 350, 120],\n",
    "    'cost': [800, 15, 50, 250, 70]\n",
    "})\n",
    "\n",
    "# Source 3: Customer data\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1, 31),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1, 31)],\n",
    "    'segment': np.random.choice(['Premium', 'Standard', 'Basic'], 30, p=[0.2, 0.5, 0.3]),\n",
    "    'join_date': pd.date_range('2024-01-01', periods=30, freq='W')\n",
    "})\n",
    "\n",
    "# TODO: Build a complete analysis pipeline:\n",
    "# 1. Merge all three datasets\n",
    "# 2. Calculate order values and profit margins\n",
    "# 3. Analyze sales by customer segment and product category\n",
    "# 4. Find top customers and products\n",
    "# 5. Calculate customer lifetime value\n",
    "# 6. Create monthly sales trend\n",
    "# 7. Identify cross-selling opportunities\n",
    "# 8. Generate executive summary DataFrame\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f420e9",
   "metadata": {},
   "source": [
    "## ðŸ“Š Lab Summary Checklist\n",
    "\n",
    "**Core Skills Practiced:**\n",
    "- [ ] GroupBy with single and multiple columns\n",
    "- [ ] Custom aggregations with agg()\n",
    "- [ ] Different types of merges (inner, left, outer)\n",
    "- [ ] Pivot tables and reshaping\n",
    "- [ ] Data cleaning and deduplication\n",
    "- [ ] Complete data pipeline\n",
    "\n",
    "**Self-Assessment:**\n",
    "- I can group and aggregate data efficiently âœ…\n",
    "- I understand different join types âœ…\n",
    "- I can reshape data between wide and long formats âœ…\n",
    "- I can clean messy real-world data âœ…\n",
    "- I can build data processing pipelines âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75456f1a",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What's Next?\n",
    "**Lab 01C:** Advanced EDA techniques and statistical analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
