{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 03 ¬∑ Notebook 02 ‚Äî From DataFrames to Databases: Mental Model Mapping\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master dataframes to databases.\n",
    "\n",
    "> Format: theory ‚Üí implementation ‚Üí best practices ‚Üí real-world application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Big Picture\n",
    "\n",
    "You've mastered pandas. You're comfortable with DataFrames. Now we're adding SQL to your toolkit.\n",
    "\n",
    "**Why both?**\n",
    "- **Pandas**: In-memory, flexible, great for exploration\n",
    "- **SQL**: Scalable, persistent, great for production\n",
    "\n",
    "Think of them as complementary tools:\n",
    "- Use SQL to **extract and reduce** data from large sources\n",
    "- Use pandas to **explore and visualize** the reduced data\n",
    "- Use SQL to **productionize** your proven analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Custom SQL display function\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def show_sql(query):\n",
    "    \"\"\"Pretty print SQL queries\"\"\"\n",
    "    display(Markdown(f\"```sql\\n{query}\\n```\"))\n",
    "\n",
    "print(\"‚úÖ Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Setting Up Our Data Laboratory\n",
    "\n",
    "We'll use the same retail dataset from Week 1, but now in both pandas AND SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample retail data (same structure as Week 1)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "n_transactions = 10000\n",
    "n_customers = 1500\n",
    "n_products = 200\n",
    "\n",
    "# Create transactions\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_transactions + 1),\n",
    "    'customer_id': np.random.randint(1, n_customers + 1, n_transactions),\n",
    "    'product_id': np.random.randint(1, n_products + 1, n_transactions),\n",
    "    'quantity': np.random.randint(1, 5, n_transactions),\n",
    "    'date': pd.date_range('2024-01-01', periods=n_transactions, freq='15min'),\n",
    "    'store_id': np.random.choice(['NYC', 'LA', 'CHI', 'HOU', 'PHX'], n_transactions)\n",
    "})\n",
    "\n",
    "# Create products\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Sports']\n",
    "products = pd.DataFrame({\n",
    "    'product_id': range(1, n_products + 1),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, n_products + 1)],\n",
    "    'category': np.random.choice(categories, n_products),\n",
    "    'price': np.round(np.random.uniform(10, 500, n_products), 2)\n",
    "})\n",
    "\n",
    "# Create customers\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1, n_customers + 1)],\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_customers),\n",
    "    'signup_date': pd.date_range('2023-01-01', periods=n_customers, freq='6H')\n",
    "})\n",
    "\n",
    "# Add revenue column\n",
    "transactions = transactions.merge(products[['product_id', 'price']], on='product_id')\n",
    "transactions['revenue'] = transactions['quantity'] * transactions['price']\n",
    "\n",
    "print(f\"üì¶ Created {len(transactions):,} transactions\")\n",
    "print(f\"üë• Created {len(customers):,} customers\")\n",
    "print(f\"üè∑Ô∏è Created {len(products):,} products\")\n",
    "\n",
    "# Preview the data\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLite database and load our data\n",
    "conn = sqlite3.connect('retail.db')\n",
    "\n",
    "# Load data into SQL\n",
    "transactions.to_sql('transactions', conn, if_exists='replace', index=False)\n",
    "products.to_sql('products', conn, if_exists='replace', index=False)\n",
    "customers.to_sql('customers', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Create a SQLAlchemy engine for pandas integration\n",
    "engine = create_engine('sqlite:///retail.db')\n",
    "\n",
    "print(\"‚úÖ Database created and data loaded!\")\n",
    "\n",
    "# Verify tables\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "print(\"\\nüìä Tables in database:\")\n",
    "for table in tables['name']:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0, 0]\n",
    "    print(f\"  - {table}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 1: Basic Operations - SELECT, WHERE, ORDER BY\n",
    "\n",
    "Let's start with the fundamentals. Every pandas operation has a SQL equivalent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Selecting Columns\n",
    "\n",
    "The most basic operation - choosing which columns to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Select specific columns\n",
    "pandas_result = transactions[['transaction_id', 'customer_id', 'revenue']].head()\n",
    "print(\"üêº Pandas approach:\")\n",
    "print(pandas_result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Select specific columns\n",
    "sql_query = \"\"\"\n",
    "SELECT transaction_id, customer_id, revenue\n",
    "FROM transactions\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL approach:\")\n",
    "show_sql(sql_query)\n",
    "sql_result = pd.read_sql(sql_query, conn)\n",
    "print(sql_result)\n",
    "\n",
    "print(\"\\n‚úÖ Results are identical!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filtering Rows (WHERE clause)\n",
    "\n",
    "Filtering is where SQL starts to shine with complex conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Multiple filter conditions\n",
    "pandas_filter = transactions[\n",
    "    (transactions['revenue'] > 500) & \n",
    "    (transactions['store_id'] == 'NYC')\n",
    "][['transaction_id', 'revenue', 'store_id']].head()\n",
    "\n",
    "print(\"üêº Pandas filtering:\")\n",
    "print(\"df[(df['revenue'] > 500) & (df['store_id'] == 'NYC')]\")\n",
    "print(pandas_filter)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: WHERE clause\n",
    "sql_query = \"\"\"\n",
    "SELECT transaction_id, revenue, store_id\n",
    "FROM transactions\n",
    "WHERE revenue > 500 \n",
    "  AND store_id = 'NYC'\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL filtering:\")\n",
    "show_sql(sql_query)\n",
    "sql_filter = pd.read_sql(sql_query, conn)\n",
    "print(sql_filter)\n",
    "\n",
    "# Pro tip comparison\n",
    "print(\"\\nüí° Pro Tip: SQL WHERE is often more readable for complex conditions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sorting (ORDER BY)\n",
    "\n",
    "Sorting is fundamental for rankings and time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Sort by multiple columns\n",
    "pandas_sorted = transactions.nlargest(10, 'revenue')[['transaction_id', 'customer_id', 'revenue']]\n",
    "\n",
    "print(\"üêº Pandas sorting (top 10 by revenue):\")\n",
    "print(\"df.nlargest(10, 'revenue')\")\n",
    "print(pandas_sorted)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: ORDER BY\n",
    "sql_query = \"\"\"\n",
    "SELECT transaction_id, customer_id, revenue\n",
    "FROM transactions\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL sorting:\")\n",
    "show_sql(sql_query)\n",
    "sql_sorted = pd.read_sql(sql_query, conn)\n",
    "print(sql_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Part 2: Aggregations - GROUP BY\n",
    "\n",
    "This is where the mental models really start to connect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Group by store and calculate metrics\n",
    "pandas_agg = transactions.groupby('store_id').agg({\n",
    "    'revenue': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"üêº Pandas aggregation:\")\n",
    "print(\"df.groupby('store_id').agg({'revenue': ['sum', 'mean', 'count']})\")\n",
    "print(pandas_agg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: GROUP BY with multiple aggregations\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    store_id,\n",
    "    SUM(revenue) as revenue_sum,\n",
    "    AVG(revenue) as revenue_mean,\n",
    "    COUNT(*) as revenue_count\n",
    "FROM transactions\n",
    "GROUP BY store_id\n",
    "ORDER BY revenue_sum DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL aggregation:\")\n",
    "show_sql(sql_query)\n",
    "sql_agg = pd.read_sql(sql_query, conn)\n",
    "print(sql_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Grouping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date components for better grouping\n",
    "transactions['date_only'] = transactions['date'].dt.date\n",
    "transactions['hour'] = transactions['date'].dt.hour\n",
    "\n",
    "# Update SQL table\n",
    "transactions.to_sql('transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# PANDAS: Multi-level groupby\n",
    "pandas_multi = transactions.groupby(['store_id', 'date_only'])['revenue'].sum().head(10)\n",
    "\n",
    "print(\"üêº Pandas multi-level groupby:\")\n",
    "print(\"df.groupby(['store_id', 'date_only'])['revenue'].sum()\")\n",
    "print(pandas_multi)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Multiple GROUP BY columns\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    store_id,\n",
    "    date_only,\n",
    "    SUM(revenue) as total_revenue\n",
    "FROM transactions\n",
    "GROUP BY store_id, date_only\n",
    "ORDER BY store_id, date_only\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL multi-level groupby:\")\n",
    "show_sql(sql_query)\n",
    "sql_multi = pd.read_sql(sql_query, conn)\n",
    "print(sql_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Filtering After Aggregation (HAVING clause)\n",
    "\n",
    "This is a key concept - filtering AFTER grouping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Filter after groupby\n",
    "store_totals = transactions.groupby('store_id')['revenue'].sum()\n",
    "pandas_having = store_totals[store_totals > 100000]\n",
    "\n",
    "print(\"üêº Pandas approach (filter after groupby):\")\n",
    "print(\"grouped = df.groupby('store_id')['revenue'].sum()\")\n",
    "print(\"grouped[grouped > 100000]\")\n",
    "print(pandas_having)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: HAVING clause\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    store_id,\n",
    "    SUM(revenue) as total_revenue\n",
    "FROM transactions\n",
    "GROUP BY store_id\n",
    "HAVING SUM(revenue) > 100000\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL approach (HAVING clause):\")\n",
    "show_sql(sql_query)\n",
    "sql_having = pd.read_sql(sql_query, conn)\n",
    "print(sql_having)\n",
    "\n",
    "print(\"\\nüí° Key Insight: WHERE filters rows BEFORE grouping, HAVING filters AFTER grouping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Part 3: JOINs - Combining Tables\n",
    "\n",
    "JOINs are SQL's superpower. Let's map them to pandas merge operations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inner Join (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Inner join\n",
    "pandas_inner = transactions[['transaction_id', 'customer_id', 'product_id', 'revenue']].merge(\n",
    "    products[['product_id', 'product_name', 'category']],\n",
    "    on='product_id',\n",
    "    how='inner'\n",
    ").head()\n",
    "\n",
    "print(\"üêº Pandas inner join:\")\n",
    "print(\"df1.merge(df2, on='product_id', how='inner')\")\n",
    "print(pandas_inner)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: INNER JOIN\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.customer_id,\n",
    "    t.product_id,\n",
    "    t.revenue,\n",
    "    p.product_name,\n",
    "    p.category\n",
    "FROM transactions t\n",
    "INNER JOIN products p ON t.product_id = p.product_id\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL inner join:\")\n",
    "show_sql(sql_query)\n",
    "sql_inner = pd.read_sql(sql_query, conn)\n",
    "print(sql_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some customers without transactions for demonstration\n",
    "all_customers = customers[['customer_id', 'customer_name']].head(10)\n",
    "customer_revenue = transactions.groupby('customer_id')['revenue'].sum().reset_index()\n",
    "\n",
    "# PANDAS: Left join\n",
    "pandas_left = all_customers.merge(\n",
    "    customer_revenue,\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ").fillna(0)\n",
    "\n",
    "print(\"üêº Pandas left join (all customers, even without purchases):\")\n",
    "print(\"customers.merge(revenue, on='customer_id', how='left').fillna(0)\")\n",
    "print(pandas_left)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: LEFT JOIN\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    COALESCE(SUM(t.revenue), 0) as total_revenue\n",
    "FROM customers c\n",
    "LEFT JOIN transactions t ON c.customer_id = t.customer_id\n",
    "WHERE c.customer_id <= 10\n",
    "GROUP BY c.customer_id, c.customer_name\n",
    "ORDER BY c.customer_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL left join:\")\n",
    "show_sql(sql_query)\n",
    "sql_left = pd.read_sql(sql_query, conn)\n",
    "print(sql_left)\n",
    "\n",
    "print(\"\\nüí° COALESCE in SQL = fillna in pandas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multiple Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Chain multiple merges\n",
    "pandas_multi_join = transactions[['transaction_id', 'customer_id', 'product_id', 'revenue']].merge(\n",
    "    products[['product_id', 'product_name', 'category']],\n",
    "    on='product_id'\n",
    ").merge(\n",
    "    customers[['customer_id', 'customer_name', 'city']],\n",
    "    on='customer_id'\n",
    ").head()\n",
    "\n",
    "print(\"üêº Pandas multiple joins:\")\n",
    "print(\"df.merge(products, on='product_id').merge(customers, on='customer_id')\")\n",
    "print(pandas_multi_join)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Multiple JOINs\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    c.customer_name,\n",
    "    c.city,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    t.revenue\n",
    "FROM transactions t\n",
    "JOIN products p ON t.product_id = p.product_id\n",
    "JOIN customers c ON t.customer_id = c.customer_id\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL multiple joins:\")\n",
    "show_sql(sql_query)\n",
    "sql_multi_join = pd.read_sql(sql_query, conn)\n",
    "print(sql_multi_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 4: Advanced Operations - Window Functions\n",
    "\n",
    "Window functions are incredibly powerful for analytics. Let's see how pandas and SQL compare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ranking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Ranking within groups\n",
    "pandas_rank = transactions.copy()\n",
    "pandas_rank['rank_in_store'] = pandas_rank.groupby('store_id')['revenue'].rank(method='dense', ascending=False)\n",
    "top_per_store = pandas_rank[pandas_rank['rank_in_store'] <= 3][['store_id', 'transaction_id', 'revenue', 'rank_in_store']].sort_values(['store_id', 'rank_in_store'])\n",
    "\n",
    "print(\"üêº Pandas ranking (top 3 transactions per store):\")\n",
    "print(\"df['rank'] = df.groupby('store_id')['revenue'].rank(method='dense', ascending=False)\")\n",
    "print(top_per_store.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Window function with RANK()\n",
    "sql_query = \"\"\"\n",
    "WITH ranked_transactions AS (\n",
    "    SELECT \n",
    "        store_id,\n",
    "        transaction_id,\n",
    "        revenue,\n",
    "        DENSE_RANK() OVER (PARTITION BY store_id ORDER BY revenue DESC) as rank_in_store\n",
    "    FROM transactions\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked_transactions\n",
    "WHERE rank_in_store <= 3\n",
    "ORDER BY store_id, rank_in_store\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL window function:\")\n",
    "show_sql(sql_query)\n",
    "sql_rank = pd.read_sql(sql_query, conn)\n",
    "print(sql_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Running Totals and Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Cumulative sum and rolling average\n",
    "daily_revenue = transactions.groupby('date_only')['revenue'].sum().reset_index()\n",
    "daily_revenue = daily_revenue.sort_values('date_only')\n",
    "daily_revenue['cumulative_revenue'] = daily_revenue['revenue'].cumsum()\n",
    "daily_revenue['moving_avg_7d'] = daily_revenue['revenue'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "print(\"üêº Pandas cumulative and rolling:\")\n",
    "print(\"df['cumsum'] = df['revenue'].cumsum()\")\n",
    "print(\"df['rolling_avg'] = df['revenue'].rolling(window=7).mean()\")\n",
    "print(daily_revenue.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Window functions for running totals\n",
    "sql_query = \"\"\"\n",
    "WITH daily_totals AS (\n",
    "    SELECT \n",
    "        date_only,\n",
    "        SUM(revenue) as daily_revenue\n",
    "    FROM transactions\n",
    "    GROUP BY date_only\n",
    ")\n",
    "SELECT \n",
    "    date_only,\n",
    "    daily_revenue,\n",
    "    SUM(daily_revenue) OVER (ORDER BY date_only) as cumulative_revenue,\n",
    "    AVG(daily_revenue) OVER (\n",
    "        ORDER BY date_only \n",
    "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "    ) as moving_avg_7d\n",
    "FROM daily_totals\n",
    "ORDER BY date_only\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL window functions:\")\n",
    "show_sql(sql_query)\n",
    "sql_window = pd.read_sql(sql_query, conn)\n",
    "print(sql_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Lead and Lag Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Shift operations for time series\n",
    "daily_revenue = transactions.groupby('date_only')['revenue'].sum().reset_index().sort_values('date_only')\n",
    "daily_revenue['prev_day_revenue'] = daily_revenue['revenue'].shift(1)\n",
    "daily_revenue['next_day_revenue'] = daily_revenue['revenue'].shift(-1)\n",
    "daily_revenue['day_over_day_change'] = daily_revenue['revenue'] - daily_revenue['prev_day_revenue']\n",
    "\n",
    "print(\"üêº Pandas shift operations:\")\n",
    "print(\"df['prev'] = df['revenue'].shift(1)\")\n",
    "print(\"df['next'] = df['revenue'].shift(-1)\")\n",
    "print(daily_revenue.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: LAG and LEAD functions\n",
    "sql_query = \"\"\"\n",
    "WITH daily_totals AS (\n",
    "    SELECT \n",
    "        date_only,\n",
    "        SUM(revenue) as daily_revenue\n",
    "    FROM transactions\n",
    "    GROUP BY date_only\n",
    ")\n",
    "SELECT \n",
    "    date_only,\n",
    "    daily_revenue,\n",
    "    LAG(daily_revenue, 1) OVER (ORDER BY date_only) as prev_day_revenue,\n",
    "    LEAD(daily_revenue, 1) OVER (ORDER BY date_only) as next_day_revenue,\n",
    "    daily_revenue - LAG(daily_revenue, 1) OVER (ORDER BY date_only) as day_over_day_change\n",
    "FROM daily_totals\n",
    "ORDER BY date_only\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL LAG/LEAD:\")\n",
    "show_sql(sql_query)\n",
    "sql_lag_lead = pd.read_sql(sql_query, conn)\n",
    "print(sql_lag_lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 5: Subqueries and CTEs\n",
    "\n",
    "Complex analytical questions often require multiple steps. Let's see how to structure them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Subqueries vs Method Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Method chaining for complex logic\n",
    "# Find customers whose average order is above the overall average\n",
    "overall_avg = transactions['revenue'].mean()\n",
    "\n",
    "pandas_complex = (\n",
    "    transactions\n",
    "    .groupby('customer_id')['revenue']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'revenue': 'avg_revenue'})\n",
    "    .query(f'avg_revenue > {overall_avg}')\n",
    "    .sort_values('avg_revenue', ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(f\"üêº Pandas: Customers with avg order > ${overall_avg:.2f}\")\n",
    "print(pandas_complex)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Using subquery\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    AVG(revenue) as avg_revenue\n",
    "FROM transactions\n",
    "GROUP BY customer_id\n",
    "HAVING AVG(revenue) > (\n",
    "    SELECT AVG(revenue) \n",
    "    FROM transactions\n",
    ")\n",
    "ORDER BY avg_revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL with subquery:\")\n",
    "show_sql(sql_query)\n",
    "sql_subquery = pd.read_sql(sql_query, conn)\n",
    "print(sql_subquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Common Table Expressions (CTEs)\n",
    "\n",
    "CTEs are like creating temporary DataFrames in your SQL query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Multi-step analysis\n",
    "# Step 1: Calculate customer metrics\n",
    "customer_metrics = transactions.groupby('customer_id').agg({\n",
    "    'revenue': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "customer_metrics.columns = ['total_revenue', 'avg_revenue', 'transaction_count']\n",
    "customer_metrics = customer_metrics.reset_index()\n",
    "\n",
    "# Step 2: Categorize customers\n",
    "customer_metrics['customer_segment'] = pd.cut(\n",
    "    customer_metrics['total_revenue'],\n",
    "    bins=[0, 1000, 5000, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Step 3: Summary by segment\n",
    "segment_summary = customer_metrics.groupby('customer_segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'total_revenue': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"üêº Pandas multi-step analysis:\")\n",
    "print(segment_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Using CTEs for the same analysis\n",
    "sql_query = \"\"\"\n",
    "WITH customer_metrics AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        SUM(revenue) as total_revenue,\n",
    "        AVG(revenue) as avg_revenue,\n",
    "        COUNT(*) as transaction_count\n",
    "    FROM transactions\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        CASE \n",
    "            WHEN total_revenue <= 1000 THEN 'Low'\n",
    "            WHEN total_revenue <= 5000 THEN 'Medium'\n",
    "            ELSE 'High'\n",
    "        END as customer_segment\n",
    "    FROM customer_metrics\n",
    ")\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    AVG(total_revenue) as avg_segment_revenue\n",
    "FROM customer_segments\n",
    "GROUP BY customer_segment\n",
    "ORDER BY avg_segment_revenue\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL with CTEs:\")\n",
    "show_sql(sql_query)\n",
    "sql_cte = pd.read_sql(sql_query, conn)\n",
    "print(sql_cte)\n",
    "\n",
    "print(\"\\nüí° CTEs make complex SQL queries readable and modular, just like method chaining in pandas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 6: Performance Considerations\n",
    "\n",
    "When should you use pandas vs SQL? Let's understand the trade-offs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Simple filtering\n",
    "print(\"üìä Test 1: Simple filtering (revenue > 100)\\n\")\n",
    "\n",
    "# Pandas timing\n",
    "start = time.time()\n",
    "pandas_result = transactions[transactions['revenue'] > 100]\n",
    "pandas_time = time.time() - start\n",
    "print(f\"üêº Pandas: {len(pandas_result):,} rows in {pandas_time:.4f} seconds\")\n",
    "\n",
    "# SQL timing\n",
    "start = time.time()\n",
    "sql_result = pd.read_sql(\"SELECT * FROM transactions WHERE revenue > 100\", conn)\n",
    "sql_time = time.time() - start\n",
    "print(f\"üóÑÔ∏è SQL: {len(sql_result):,} rows in {sql_time:.4f} seconds\")\n",
    "\n",
    "print(f\"\\n‚ö° Faster: {'Pandas' if pandas_time < sql_time else 'SQL'} by {abs(pandas_time - sql_time):.4f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test 2: Complex aggregation\n",
    "print(\"üìä Test 2: Complex aggregation (group by store, calculate multiple metrics)\\n\")\n",
    "\n",
    "# Pandas timing\n",
    "start = time.time()\n",
    "pandas_agg = transactions.groupby('store_id').agg({\n",
    "    'revenue': ['sum', 'mean', 'std'],\n",
    "    'quantity': ['sum', 'mean'],\n",
    "    'transaction_id': 'count'\n",
    "})\n",
    "pandas_time = time.time() - start\n",
    "print(f\"üêº Pandas: Aggregated in {pandas_time:.4f} seconds\")\n",
    "\n",
    "# SQL timing\n",
    "start = time.time()\n",
    "sql_agg = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        store_id,\n",
    "        SUM(revenue) as revenue_sum,\n",
    "        AVG(revenue) as revenue_mean,\n",
    "        SUM(quantity) as quantity_sum,\n",
    "        AVG(quantity) as quantity_mean,\n",
    "        COUNT(*) as transaction_count\n",
    "    FROM transactions\n",
    "    GROUP BY store_id\n",
    "\"\"\", conn)\n",
    "sql_time = time.time() - start\n",
    "print(f\"üóÑÔ∏è SQL: Aggregated in {sql_time:.4f} seconds\")\n",
    "\n",
    "print(f\"\\n‚ö° Faster: {'Pandas' if pandas_time < sql_time else 'SQL'} by {abs(pandas_time - sql_time):.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä When to Use Each Tool\n",
    "\n",
    "Based on our experiments and real-world experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_matrix = pd.DataFrame({\n",
    "    'Scenario': [\n",
    "        'Data exploration & prototyping',\n",
    "        'Production data pipelines',\n",
    "        'Complex statistical analysis',\n",
    "        'Data > 1GB',\n",
    "        'Real-time dashboards',\n",
    "        'Ad-hoc business queries',\n",
    "        'Machine learning features',\n",
    "        'Data validation & cleaning',\n",
    "        'Time series manipulation',\n",
    "        'Joining multiple large tables'\n",
    "    ],\n",
    "    'Preferred Tool': [\n",
    "        'üêº Pandas',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üêº Pandas',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üêº Pandas ‚Üí SQL',\n",
    "        'üêº Pandas',\n",
    "        'üêº Pandas',\n",
    "        'üóÑÔ∏è SQL'\n",
    "    ],\n",
    "    'Reason': [\n",
    "        'Interactive, flexible, great for iteration',\n",
    "        'Scalable, auditable, version-controlled',\n",
    "        'Rich statistical libraries (scipy, statsmodels)',\n",
    "        'Memory constraints, let database do the work',\n",
    "        'Direct queries, no data movement',\n",
    "        'Standard language, shareable queries',\n",
    "        'Prototype in pandas, productionize in SQL',\n",
    "        'Better string/regex operations',\n",
    "        'Superior datetime handling',\n",
    "        'Optimized query planner'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"üéØ DECISION MATRIX: When to Use Pandas vs SQL\\n\")\n",
    "for _, row in decision_matrix.iterrows():\n",
    "    print(f\"{row['Preferred Tool']} {row['Scenario']}\")\n",
    "    print(f\"    ‚Üí {row['Reason']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Part 7: Hybrid Workflows - Best of Both Worlds\n",
    "\n",
    "The real power comes from combining pandas and SQL seamlessly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Using SQL for Data Reduction, Pandas for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid approach: Let SQL do the heavy lifting, pandas for fine-tuning\n",
    "\n",
    "# Step 1: Use SQL to filter and aggregate large data\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    date_only,\n",
    "    store_id,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    SUM(revenue) as total_revenue,\n",
    "    AVG(revenue) as avg_transaction\n",
    "FROM transactions\n",
    "WHERE revenue > 50  -- Pre-filter in SQL\n",
    "GROUP BY date_only, store_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"Step 1: SQL for heavy lifting\")\n",
    "show_sql(sql_query)\n",
    "\n",
    "# Execute and get results\n",
    "daily_store_metrics = pd.read_sql(sql_query, conn)\n",
    "print(f\"\\n‚úÖ Reduced to {len(daily_store_metrics):,} rows\\n\")\n",
    "\n",
    "# Step 2: Use pandas for complex transformations\n",
    "print(\"Step 2: Pandas for complex analysis\")\n",
    "\n",
    "# Convert to datetime\n",
    "daily_store_metrics['date_only'] = pd.to_datetime(daily_store_metrics['date_only'])\n",
    "\n",
    "# Add time-based features\n",
    "daily_store_metrics['day_of_week'] = daily_store_metrics['date_only'].dt.day_name()\n",
    "daily_store_metrics['is_weekend'] = daily_store_metrics['date_only'].dt.dayofweek.isin([5, 6])\n",
    "\n",
    "# Calculate store performance ranking by day\n",
    "daily_store_metrics['daily_rank'] = daily_store_metrics.groupby('date_only')['total_revenue'].rank(ascending=False)\n",
    "\n",
    "# Show results\n",
    "print(daily_store_metrics.head(10))\n",
    "\n",
    "print(\"\\nüí° Best Practice: Use SQL to reduce data volume, pandas for complex transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Parameterized Queries from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe parameterized queries - avoid SQL injection!\n",
    "\n",
    "def get_customer_history(customer_id, min_revenue=0):\n",
    "    \"\"\"\n",
    "    Safely query customer transaction history\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.date,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        t.revenue\n",
    "    FROM transactions t\n",
    "    JOIN products p ON t.product_id = p.product_id\n",
    "    WHERE t.customer_id = ?\n",
    "      AND t.revenue > ?\n",
    "    ORDER BY t.date DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use parameterized query for safety\n",
    "    return pd.read_sql(query, conn, params=(customer_id, min_revenue))\n",
    "\n",
    "# Example usage\n",
    "customer_data = get_customer_history(customer_id=42, min_revenue=100)\n",
    "print(\"üîí Safe parameterized query result:\")\n",
    "print(customer_data)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è NEVER use string formatting for SQL queries - always use parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Pushing Pandas Operations to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes it's better to push operations to the database\n",
    "\n",
    "# Scenario: Complex filtering that could be done in either tool\n",
    "stores_of_interest = ['NYC', 'LA']\n",
    "date_range = ('2024-01-01', '2024-01-07')\n",
    "\n",
    "print(\"Approach 1: Load all data, filter in pandas (DON'T DO THIS)\")\n",
    "print(\"```python\")\n",
    "print(\"# This loads ALL data into memory first!\")\n",
    "print(\"df = pd.read_sql('SELECT * FROM transactions', conn)\")\n",
    "print(\"df_filtered = df[(df['store_id'].isin(stores)) & (df['date'] >= start)]\")\n",
    "print(\"```\\n\")\n",
    "\n",
    "print(\"Approach 2: Filter in SQL (DO THIS)\")\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM transactions\n",
    "WHERE store_id IN ({','.join(['?'] * len(stores_of_interest))})\n",
    "  AND date >= ?\n",
    "  AND date <= ?\n",
    "\"\"\"\n",
    "\n",
    "show_sql(query.replace('?', \"'store_name'\"))\n",
    "\n",
    "# Execute with parameters\n",
    "filtered_data = pd.read_sql(\n",
    "    query, \n",
    "    conn, \n",
    "    params=stores_of_interest + list(date_range)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded only {len(filtered_data):,} relevant rows instead of {len(transactions):,}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises using BOTH pandas and SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Customer Segmentation\n",
    "\n",
    "Find the top 10% of customers by total spending and analyze their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your pandas solution here\n",
    "# Hint: Use quantile() to find the 90th percentile threshold\n",
    "\n",
    "# pandas_solution = ...\n",
    "\n",
    "print(\"üêº Your pandas solution:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your SQL solution here\n",
    "# Hint: Use NTILE() or calculate percentiles with window functions\n",
    "\n",
    "sql_query = \"\"\"\n",
    "-- Your SQL here\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è Your SQL solution:\")\n",
    "# sql_solution = pd.read_sql(sql_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cohort Analysis\n",
    "\n",
    "Calculate retention by customer signup month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a cohort analysis\n",
    "# 1. Group customers by signup month\n",
    "# 2. Track their activity in subsequent months\n",
    "# 3. Calculate retention rates\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Product Affinity\n",
    "\n",
    "Find which products are frequently bought together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement market basket analysis\n",
    "# Find products that appear in the same transactions\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "1. **Mental Model Mapping**:\n",
    "   - `df.groupby()` ‚Üí `GROUP BY`\n",
    "   - `df.merge()` ‚Üí `JOIN`\n",
    "   - `df['col'].rank()` ‚Üí `RANK() OVER()`\n",
    "   - `df['col'].shift()` ‚Üí `LAG()/LEAD()`\n",
    "\n",
    "2. **When to Use SQL**:\n",
    "   - Data is in a database (avoid loading unnecessary data)\n",
    "   - Need to share queries with non-Python users\n",
    "   - Production pipelines requiring audit trails\n",
    "   - Working with data larger than memory\n",
    "\n",
    "3. **When to Use Pandas**:\n",
    "   - Exploratory data analysis\n",
    "   - Complex statistical operations\n",
    "   - Data cleaning and string manipulation\n",
    "   - Visualization preparation\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Use SQL to reduce data volume first\n",
    "   - Always use parameterized queries\n",
    "   - Think in sets (SQL) vs iterations (pandas)\n",
    "   - Document complex queries with CTEs\n",
    "   - Profile performance for large datasets\n",
    "\n",
    "5. **Hybrid Approach**:\n",
    "   - SQL for extraction and reduction\n",
    "   - Pandas for transformation and analysis\n",
    "   - SQL for productionization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "In the next notebook, we'll dive deeper into:\n",
    "- Data warehouse design patterns\n",
    "- Star and snowflake schemas\n",
    "- Optimizing query performance\n",
    "- Working with cloud data warehouses\n",
    "\n",
    "Remember: **You don't choose pandas OR SQL - you master BOTH!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "conn.close()\n",
    "print(\"‚úÖ Database connection closed. Great work!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
