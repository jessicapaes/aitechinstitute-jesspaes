{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 03 ¬∑ Notebook 04 ‚Äî SQL Data Pipeline: Building Real-World Databases üèóÔ∏è\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master the complete data pipeline from multiple sources to unified database\n",
    "> Format: theory ‚Üí implementation ‚Üí best practices ‚Üí real-world application.\n",
    ">\n",
    "**Learning Objectives:**\n",
    "- Create DataFrames from scratch and convert to SQL databases\n",
    "- Read CSV files and handle data cleaning during import\n",
    "- Merge data from multiple databases into a unified warehouse\n",
    "- Build production-ready data pipelines with proper error handling\n",
    "- Export and backup database content for sharing and analysis\n",
    "\n",
    "**Time:** 90 minutes | **Prerequisites:** Basic SQL knowledge (complete Notebook 00 first)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Real-World Challenge\n",
    "\n",
    "You're the new Data Engineer at an e-commerce company. Your data is scattered:\n",
    "- üìä Sales data in Python DataFrames\n",
    "- üìù Customer data in CSV files\n",
    "- üóÑÔ∏è Product data in different databases\n",
    "- üìà Marketing data being created daily\n",
    "\n",
    "**Your mission:** Build a unified database that combines everything!\n",
    "\n",
    "This notebook teaches you exactly how to do this professionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Helper functions for database operations\n",
    "def show_table_info(conn, table_name):\n",
    "    \"\"\"Display information about a table\"\"\"\n",
    "    try:\n",
    "        count = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table_name}\", conn).iloc[0, 0]\n",
    "        columns = pd.read_sql(f\"PRAGMA table_info({table_name})\", conn)\n",
    "        print(f\"üìä Table: {table_name}\")\n",
    "        print(f\"   Rows: {count:,}\")\n",
    "        print(f\"   Columns: {', '.join(columns['name'].tolist())}\")\n",
    "    except:\n",
    "        print(f\"‚ùå Table {table_name} not found\")\n",
    "\n",
    "def list_all_tables(conn):\n",
    "    \"\"\"List all tables in database\"\"\"\n",
    "    tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "    return tables['name'].tolist()\n",
    "\n",
    "print(\"‚úÖ Environment ready for data pipeline building!\")\n",
    "print(\"üìö Libraries loaded: pandas, numpy, sqlite3, matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Creating DataFrames from Scratch ‚Üí SQL\n",
    "\n",
    "### Theory\n",
    "Converting DataFrames to SQL databases is essential because:\n",
    "- **Persistence**: Data survives beyond Python session\n",
    "- **Scalability**: Handle data larger than RAM\n",
    "- **Sharing**: Multiple users can access simultaneously\n",
    "- **Performance**: Optimized queries on indexed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implementation: Creating Related DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è CREATING DATAFRAMES FROM SCRATCH\\n\")\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Create CUSTOMERS DataFrame\n",
    "print(\"1Ô∏è‚É£ Creating Customers Data...\")\n",
    "n_customers = 1000\n",
    "\n",
    "customers_df = pd.DataFrame({\n",
    "    'customer_id': range(1001, 1001 + n_customers),\n",
    "    'first_name': [f'First_{i}' for i in range(n_customers)],\n",
    "    'last_name': [f'Last_{i}' for i in range(n_customers)],\n",
    "    'email': [f'customer{i}@email.com' for i in range(n_customers)],\n",
    "    'country': np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'France'], n_customers, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
    "    'registration_date': pd.date_range('2020-01-01', periods=n_customers, freq='6H'),\n",
    "    'customer_segment': np.random.choice(['Gold', 'Silver', 'Bronze'], n_customers, p=[0.2, 0.3, 0.5]),\n",
    "    'lifetime_value': np.random.exponential(500, n_customers).round(2)\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Created {len(customers_df):,} customers\")\n",
    "print(customers_df.head(3))\n",
    "print()\n",
    "\n",
    "# Create PRODUCTS DataFrame\n",
    "print(\"2Ô∏è‚É£ Creating Products Data...\")\n",
    "n_products = 500\n",
    "\n",
    "products_df = pd.DataFrame({\n",
    "    'product_id': [f'PROD_{i:04d}' for i in range(1, n_products + 1)],\n",
    "    'product_name': [f'Product {i}' for i in range(1, n_products + 1)],\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Sports', 'Books'], n_products),\n",
    "    'price': np.random.uniform(10, 1000, n_products).round(2),\n",
    "    'cost': np.random.uniform(5, 500, n_products).round(2),\n",
    "    'stock_quantity': np.random.randint(0, 1000, n_products)\n",
    "})\n",
    "\n",
    "products_df['margin_percent'] = ((products_df['price'] - products_df['cost']) / products_df['price'] * 100).round(2)\n",
    "\n",
    "print(f\"‚úÖ Created {len(products_df):,} products\")\n",
    "print(products_df.head(3))\n",
    "print()\n",
    "\n",
    "# Create TRANSACTIONS DataFrame\n",
    "print(\"3Ô∏è‚É£ Creating Transactions Data...\")\n",
    "n_transactions = 10000\n",
    "\n",
    "transactions_df = pd.DataFrame({\n",
    "    'transaction_id': range(5001, 5001 + n_transactions),\n",
    "    'customer_id': np.random.choice(customers_df['customer_id'], n_transactions),\n",
    "    'product_id': np.random.choice(products_df['product_id'], n_transactions),\n",
    "    'transaction_date': pd.date_range('2023-01-01', periods=n_transactions, freq='15min'),\n",
    "    'quantity': np.random.randint(1, 5, n_transactions),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'PayPal', 'Bank Transfer'], n_transactions, p=[0.5, 0.3, 0.2])\n",
    "})\n",
    "\n",
    "# Calculate total amount\n",
    "transactions_df = transactions_df.merge(products_df[['product_id', 'price']], on='product_id', how='left')\n",
    "transactions_df['total_amount'] = (transactions_df['quantity'] * transactions_df['price']).round(2)\n",
    "\n",
    "print(f\"‚úÖ Created {len(transactions_df):,} transactions\")\n",
    "print(transactions_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Best Practices: Saving DataFrames to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ SAVING DATAFRAMES TO SQL DATABASE\\n\")\n",
    "\n",
    "# Create database connection\n",
    "company_db = sqlite3.connect('company_database.db')\n",
    "\n",
    "# Best Practice: Define data types explicitly\n",
    "print(\"üìå Best Practice: Specify data types for better performance\\n\")\n",
    "\n",
    "# Save customers with proper constraints\n",
    "customers_df.to_sql(\n",
    "    name='customers',\n",
    "    con=company_db,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype={\n",
    "        'customer_id': 'INTEGER PRIMARY KEY',\n",
    "        'email': 'TEXT UNIQUE',\n",
    "        'registration_date': 'DATETIME'\n",
    "    }\n",
    ")\n",
    "print(\"‚úÖ Customers table created with constraints\")\n",
    "\n",
    "# Save products\n",
    "products_df.to_sql(\n",
    "    name='products',\n",
    "    con=company_db,\n",
    "    if_exists='replace',\n",
    "    index=False\n",
    ")\n",
    "print(\"‚úÖ Products table created\")\n",
    "\n",
    "# Save transactions\n",
    "transactions_df.to_sql(\n",
    "    name='transactions',\n",
    "    con=company_db,\n",
    "    if_exists='replace',\n",
    "    index=False\n",
    ")\n",
    "print(\"‚úÖ Transactions table created\")\n",
    "\n",
    "# Verify database\n",
    "print(\"\\nüìä Database Summary:\")\n",
    "for table in list_all_tables(company_db):\n",
    "    show_table_info(company_db, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÅ Part 2: Reading CSV Files ‚Üí SQL Database\n",
    "\n",
    "### Theory\n",
    "CSV files are the most common data exchange format. Key challenges:\n",
    "- **Data types**: CSV doesn't preserve types\n",
    "- **Missing values**: Various representations (empty, NULL, N/A)\n",
    "- **Inconsistent formats**: Dates, numbers with commas\n",
    "- **Encoding issues**: UTF-8, Latin-1, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementation: Creating Sample CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù CREATING SAMPLE CSV FILES\\n\")\n",
    "\n",
    "# Create directory for CSV files\n",
    "os.makedirs('csv_data', exist_ok=True)\n",
    "\n",
    "# Create Marketing Campaign CSV\n",
    "marketing_data = pd.DataFrame({\n",
    "    'campaign_id': [f'CAMP_{i:04d}' for i in range(1, 101)],\n",
    "    'campaign_name': [f'Campaign {i}' for i in range(1, 101)],\n",
    "    'start_date': pd.date_range('2023-01-01', periods=100, freq='3D'),\n",
    "    'channel': np.random.choice(['Email', 'Social Media', 'TV', 'Online'], 100),\n",
    "    'budget': np.random.uniform(1000, 50000, 100).round(2),\n",
    "    'conversion_rate': np.random.uniform(0.01, 0.15, 100).round(4)\n",
    "})\n",
    "\n",
    "marketing_data.to_csv('csv_data/marketing_campaigns.csv', index=False)\n",
    "print(f\"‚úÖ Created marketing_campaigns.csv with {len(marketing_data)} rows\")\n",
    "\n",
    "# Create Supplier CSV\n",
    "supplier_data = pd.DataFrame({\n",
    "    'supplier_id': range(1, 51),\n",
    "    'supplier_name': [f'Supplier Corp {i}' for i in range(1, 51)],\n",
    "    'country': np.random.choice(['China', 'India', 'USA', 'Mexico'], 50),\n",
    "    'rating': np.random.uniform(3.0, 5.0, 50).round(1),\n",
    "    'delivery_days': np.random.randint(1, 30, 50)\n",
    "})\n",
    "\n",
    "supplier_data.to_csv('csv_data/suppliers.csv', index=False)\n",
    "print(f\"‚úÖ Created suppliers.csv with {len(supplier_data)} rows\")\n",
    "\n",
    "print(\"\\nüìÅ CSV files created in 'csv_data' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Best Practices: Data Cleaning During Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßπ DATA CLEANING DURING CSV IMPORT\\n\")\n",
    "\n",
    "# Create a messy CSV to demonstrate cleaning\n",
    "messy_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3, None, 5],\n",
    "    'name': ['Alice', 'Bob', '', 'Diana', None],\n",
    "    'age': [25, -5, 150, 30, 35],\n",
    "    'email': ['alice@email.com', 'invalid-email', 'charlie@email.com', None, 'eve@email.com'],\n",
    "    'salary': ['50000', '60,000', '$70000', '80000', 'not_a_number']\n",
    "})\n",
    "\n",
    "messy_data.to_csv('csv_data/messy_data.csv', index=False)\n",
    "print(\"Created messy_data.csv for demonstration\\n\")\n",
    "\n",
    "def clean_and_import(csv_file, connection, table_name):\n",
    "    \"\"\"Clean data before importing to SQL\"\"\"\n",
    "    print(f\"Cleaning {csv_file}...\")\n",
    "    \n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Cleaning operations\n",
    "    cleaning_log = []\n",
    "    \n",
    "    # 1. Remove rows with missing IDs\n",
    "    if 'id' in df.columns:\n",
    "        df = df.dropna(subset=['id'])\n",
    "        cleaning_log.append(f\"Removed {original_shape[0] - len(df)} rows with missing IDs\")\n",
    "    \n",
    "    # 2. Fill missing names\n",
    "    if 'name' in df.columns:\n",
    "        df['name'] = df['name'].fillna('Unknown').replace('', 'Unknown')\n",
    "        cleaning_log.append(\"Filled missing names\")\n",
    "    \n",
    "    # 3. Validate age\n",
    "    if 'age' in df.columns:\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "        df.loc[(df['age'] < 0) | (df['age'] > 120), 'age'] = None\n",
    "        cleaning_log.append(\"Validated age values\")\n",
    "    \n",
    "    # 4. Validate email\n",
    "    if 'email' in df.columns:\n",
    "        df.loc[~df['email'].astype(str).str.contains('@', na=False), 'email'] = None\n",
    "        cleaning_log.append(\"Validated email addresses\")\n",
    "    \n",
    "    # 5. Clean salary\n",
    "    if 'salary' in df.columns:\n",
    "        df['salary'] = df['salary'].astype(str).str.replace('[$,]', '', regex=True)\n",
    "        df['salary'] = pd.to_numeric(df['salary'], errors='coerce')\n",
    "        cleaning_log.append(\"Cleaned salary values\")\n",
    "    \n",
    "    print(\"Cleaning operations:\")\n",
    "    for op in cleaning_log:\n",
    "        print(f\"  ‚Ä¢ {op}\")\n",
    "    \n",
    "    # Save to database\n",
    "    df.to_sql(table_name, connection, if_exists='replace', index=False)\n",
    "    print(f\"‚úÖ Saved to table '{table_name}'\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create new database for CSV data\n",
    "csv_db = sqlite3.connect('csv_database.db')\n",
    "\n",
    "# Import with cleaning\n",
    "cleaned_df = clean_and_import('csv_data/messy_data.csv', csv_db, 'cleaned_data')\n",
    "print(\"Cleaned data sample:\")\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Real-World Application: Importing Multiple CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• IMPORTING MULTIPLE CSV FILES\\n\")\n",
    "\n",
    "def csv_to_sql(csv_file, table_name, connection):\n",
    "    \"\"\"Read CSV and save to SQL database\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Convert date columns\n",
    "    for col in df.columns:\n",
    "        if 'date' in col.lower():\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    df.to_sql(table_name, connection, if_exists='replace', index=False)\n",
    "    print(f\"‚úÖ Imported {table_name}: {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "# Import all CSV files\n",
    "csv_files = [\n",
    "    ('csv_data/marketing_campaigns.csv', 'marketing'),\n",
    "    ('csv_data/suppliers.csv', 'suppliers')\n",
    "]\n",
    "\n",
    "for csv_file, table_name in csv_files:\n",
    "    csv_to_sql(csv_file, table_name, csv_db)\n",
    "\n",
    "print(\"\\nüìä CSV Database Summary:\")\n",
    "for table in list_all_tables(csv_db):\n",
    "    show_table_info(csv_db, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 3: Merging Multiple Databases\n",
    "\n",
    "### Theory\n",
    "Data warehousing combines multiple sources into a unified analytical database:\n",
    "- **Dimensional modeling**: Facts and dimensions\n",
    "- **ETL process**: Extract, Transform, Load\n",
    "- **Data consistency**: Unified formats and standards\n",
    "- **Performance optimization**: Aggregate tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implementation: Reading from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö READING FROM MULTIPLE DATABASES\\n\")\n",
    "\n",
    "# Load data from company database\n",
    "print(\"From company_database.db:\")\n",
    "company_customers = pd.read_sql(\"SELECT * FROM customers\", company_db)\n",
    "company_products = pd.read_sql(\"SELECT * FROM products\", company_db)\n",
    "company_transactions = pd.read_sql(\"SELECT * FROM transactions\", company_db)\n",
    "print(f\"  ‚úÖ Loaded {len(company_customers)} customers\")\n",
    "print(f\"  ‚úÖ Loaded {len(company_products)} products\")\n",
    "print(f\"  ‚úÖ Loaded {len(company_transactions)} transactions\")\n",
    "\n",
    "# Load data from CSV database\n",
    "print(\"\\nFrom csv_database.db:\")\n",
    "csv_marketing = pd.read_sql(\"SELECT * FROM marketing\", csv_db)\n",
    "csv_suppliers = pd.read_sql(\"SELECT * FROM suppliers\", csv_db)\n",
    "print(f\"  ‚úÖ Loaded {len(csv_marketing)} marketing campaigns\")\n",
    "print(f\"  ‚úÖ Loaded {len(csv_suppliers)} suppliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Best Practices: Creating Unified Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è CREATING UNIFIED DATA WAREHOUSE\\n\")\n",
    "\n",
    "# Create new data warehouse\n",
    "warehouse_db = sqlite3.connect('data_warehouse.db')\n",
    "\n",
    "print(\"Creating dimensional model...\\n\")\n",
    "\n",
    "# Create Date Dimension\n",
    "date_range = pd.date_range('2020-01-01', '2025-12-31', freq='D')\n",
    "dim_date = pd.DataFrame({\n",
    "    'date_key': date_range.strftime('%Y%m%d').astype(int),\n",
    "    'date': date_range,\n",
    "    'year': date_range.year,\n",
    "    'month': date_range.month,\n",
    "    'month_name': date_range.strftime('%B'),\n",
    "    'day_name': date_range.strftime('%A'),\n",
    "    'is_weekend': (date_range.dayofweek >= 5).astype(int)\n",
    "})\n",
    "dim_date.to_sql('dim_date', warehouse_db, if_exists='replace', index=False)\n",
    "print(f\"‚úÖ Created dim_date: {len(dim_date)} dates\")\n",
    "\n",
    "# Create Customer Dimension\n",
    "dim_customer = company_customers.copy()\n",
    "dim_customer.to_sql('dim_customer', warehouse_db, if_exists='replace', index=False)\n",
    "print(f\"‚úÖ Created dim_customer: {len(dim_customer)} customers\")\n",
    "\n",
    "# Create Product Dimension (enriched with supplier info)\n",
    "dim_product = company_products.copy()\n",
    "# Add supplier mapping\n",
    "dim_product['supplier_id'] = (dim_product.index % len(csv_suppliers)) + 1\n",
    "dim_product = dim_product.merge(\n",
    "    csv_suppliers[['supplier_id', 'supplier_name', 'country']],\n",
    "    on='supplier_id',\n",
    "    how='left'\n",
    ")\n",
    "dim_product.to_sql('dim_product', warehouse_db, if_exists='replace', index=False)\n",
    "print(f\"‚úÖ Created dim_product: {len(dim_product)} products\")\n",
    "\n",
    "# Create Fact Table\n",
    "fact_sales = company_transactions.copy()\n",
    "fact_sales['date_key'] = pd.to_datetime(fact_sales['transaction_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "fact_sales.to_sql('fact_sales', warehouse_db, if_exists='replace', index=False)\n",
    "print(f\"‚úÖ Created fact_sales: {len(fact_sales)} transactions\")\n",
    "\n",
    "print(\"\\nüìä Data Warehouse Structure:\")\n",
    "for table in list_all_tables(warehouse_db):\n",
    "    show_table_info(warehouse_db, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Real-World Application: Creating Aggregate Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà CREATING AGGREGATE TABLES FOR PERFORMANCE\\n\")\n",
    "\n",
    "# Daily Sales Summary\n",
    "daily_sales_query = \"\"\"\n",
    "SELECT \n",
    "    d.date,\n",
    "    d.month_name,\n",
    "    d.day_name,\n",
    "    COUNT(DISTINCT f.customer_id) as unique_customers,\n",
    "    COUNT(f.transaction_id) as num_transactions,\n",
    "    SUM(f.total_amount) as total_revenue\n",
    "FROM fact_sales f\n",
    "JOIN dim_date d ON f.date_key = d.date_key\n",
    "GROUP BY d.date, d.month_name, d.day_name\n",
    "\"\"\"\n",
    "\n",
    "daily_sales = pd.read_sql(daily_sales_query, warehouse_db)\n",
    "daily_sales.to_sql('agg_daily_sales', warehouse_db, if_exists='replace', index=False)\n",
    "print(f\"‚úÖ Created agg_daily_sales: {len(daily_sales)} daily summaries\")\n",
    "\n",
    "# Customer Lifetime Value\n",
    "clv_query = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_segment,\n",
    "    c.country,\n",
    "    COUNT(f.transaction_id) as total_transactions,\n",
    "    SUM(f.total_amount) as lifetime_value\n",
    "FROM dim_customer c\n",
    "LEFT JOIN fact_sales f ON c.customer_id = f.customer_id\n",
    "GROUP BY c.customer_id, c.customer_segment, c.country\n",
    "\"\"\"\n",
    "\n",
    "customer_clv = pd.read_sql(clv_query, warehouse_db)\n",
    "customer_clv.to_sql('agg_customer_ltv', warehouse_db, if_exists='replace', index=False)\n",
    "print(f\"‚úÖ Created agg_customer_ltv: {len(customer_clv)} customer summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 4: Analytics on Unified Database\n",
    "\n",
    "### Real-World Application: Business Intelligence Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä BUSINESS INTELLIGENCE ANALYTICS\\n\")\n",
    "\n",
    "# Analysis 1: Sales by Day of Week\n",
    "day_analysis = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        day_name,\n",
    "        AVG(total_revenue) as avg_revenue,\n",
    "        AVG(num_transactions) as avg_transactions\n",
    "    FROM agg_daily_sales\n",
    "    GROUP BY day_name\n",
    "    ORDER BY avg_revenue DESC\n",
    "\"\"\", warehouse_db)\n",
    "\n",
    "print(\"Sales Performance by Day of Week:\")\n",
    "print(day_analysis)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].bar(day_analysis['day_name'], day_analysis['avg_revenue'], color='steelblue')\n",
    "ax[0].set_title('Average Revenue by Day')\n",
    "ax[0].set_xlabel('Day of Week')\n",
    "ax[0].set_ylabel('Revenue ($)')\n",
    "ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax[1].bar(day_analysis['day_name'], day_analysis['avg_transactions'], color='coral')\n",
    "ax[1].set_title('Average Transactions by Day')\n",
    "ax[1].set_xlabel('Day of Week')\n",
    "ax[1].set_ylabel('# Transactions')\n",
    "ax[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis 2: Customer Segmentation\n",
    "segment_analysis = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        COUNT(*) as customer_count,\n",
    "        AVG(lifetime_value) as avg_ltv\n",
    "    FROM agg_customer_ltv\n",
    "    WHERE lifetime_value > 0\n",
    "    GROUP BY customer_segment\n",
    "    ORDER BY avg_ltv DESC\n",
    "\"\"\", warehouse_db)\n",
    "\n",
    "print(\"\\nCustomer Segmentation Analysis:\")\n",
    "print(segment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Part 5: Exporting and Backing Up\n",
    "\n",
    "### Best Practices: Data Export and Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ EXPORTING DATABASE CONTENT\\n\")\n",
    "\n",
    "# Create export directory\n",
    "os.makedirs('warehouse_export', exist_ok=True)\n",
    "\n",
    "# Export key tables\n",
    "tables_to_export = ['dim_customer', 'dim_product', 'agg_daily_sales']\n",
    "\n",
    "for table in tables_to_export:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table}\", warehouse_db)\n",
    "    filename = f'warehouse_export/{table}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Exported {table}: {len(df)} rows\")\n",
    "\n",
    "# Create summary report\n",
    "total_revenue = pd.read_sql(\"SELECT SUM(total_amount) as rev FROM fact_sales\", warehouse_db).iloc[0, 0]\n",
    "total_customers = pd.read_sql(\"SELECT COUNT(*) as cnt FROM dim_customer\", warehouse_db).iloc[0, 0]\n",
    "\n",
    "summary = f\"\"\"\n",
    "DATA WAREHOUSE SUMMARY\n",
    "{'='*30}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "Total Revenue: ${total_revenue:,.2f}\n",
    "Total Customers: {total_customers:,}\n",
    "Database: data_warehouse.db\n",
    "\"\"\"\n",
    "\n",
    "with open('warehouse_export/summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\nüìÅ Export complete! Files saved in 'warehouse_export/'\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù PRACTICE EXERCISES\\n\")\n",
    "\n",
    "print(\"Exercise 1: Create Your Own Pipeline\")\n",
    "print(\"  ‚Ä¢ Create 3 related DataFrames\")\n",
    "print(\"  ‚Ä¢ Save to a new database\")\n",
    "print(\"  ‚Ä¢ Query for insights\")\n",
    "print()\n",
    "\n",
    "print(\"Exercise 2: Data Cleaning Challenge\")\n",
    "print(\"  ‚Ä¢ Create a messy CSV with various issues\")\n",
    "print(\"  ‚Ä¢ Write cleaning functions\")\n",
    "print(\"  ‚Ä¢ Import to clean database\")\n",
    "print()\n",
    "\n",
    "print(\"Exercise 3: Database Merge\")\n",
    "print(\"  ‚Ä¢ Create two separate databases\")\n",
    "print(\"  ‚Ä¢ Merge into unified warehouse\")\n",
    "print(\"  ‚Ä¢ Create aggregate tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "You've mastered critical data engineering skills:\n",
    "\n",
    "1. **DataFrame ‚Üí SQL**: Convert Python data to persistent databases\n",
    "2. **CSV ‚Üí SQL**: Import and clean external data\n",
    "3. **Database Merging**: Combine multiple sources\n",
    "4. **Data Warehousing**: Build dimensional models\n",
    "5. **ETL Pipelines**: Extract, Transform, Load workflows\n",
    "6. **Data Cleaning**: Handle real-world data issues\n",
    "7. **Performance**: Create aggregate tables\n",
    "8. **Export**: Backup and share results\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "You're now ready to:\n",
    "- Build production data pipelines\n",
    "- Handle real-world data integration\n",
    "- Create analytical databases\n",
    "- Implement ETL processes\n",
    "\n",
    "**Remember**: This is how real data engineering works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "company_db.close()\n",
    "csv_db.close()\n",
    "warehouse_db.close()\n",
    "\n",
    "print(\"‚úÖ All database connections closed.\")\n",
    "print(\"üéâ Congratulations! You've built a complete data pipeline!\")\n",
    "print(\"\")\n",
    "print(\"üìä You created:\")\n",
    "print(\"  - 3 separate databases\")\n",
    "print(\"  - 10+ tables\")\n",
    "print(\"  - 10,000+ records\")\n",
    "print(\"  - 1 unified data warehouse\")\n",
    "print(\"\")\n",
    "print(\"üí™ You're now ready for real-world data engineering!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
