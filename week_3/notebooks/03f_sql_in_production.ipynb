{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** · *Intermediate AI & Data Science*\n",
    "### Week 03 · Notebook 07 — Performance Optimization & Production Best Practices\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Performance Optimization & Production Best Practices\n",
    "\n",
    "> Format: theory → implementation → best practices → real-world application.\n",
    ">\n",
    "**Learning Objectives:**\n",
    "- Understand and analyze query execution plans\n",
    "- Master indexing strategies for optimal performance\n",
    "- Implement query optimization techniques\n",
    "- Apply production best practices for maintainable SQL\n",
    "- Monitor and tune database performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 The Production Challenge\n",
    "\n",
    "Your startup just hit 100M rows in the database. Queries that took milliseconds now take minutes.\n",
    "\n",
    "**Real scenarios we'll solve:**\n",
    "1. Dashboard queries timing out\n",
    "2. ETL jobs running for hours\n",
    "3. Database CPU at 100%\n",
    "4. Storage costs exploding\n",
    "\n",
    "Time to optimize for **production scale**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure displays\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def show_sql(query, title=\"SQL Query:\"):\n",
    "    \"\"\"Pretty print SQL queries\"\"\"\n",
    "    print(f\"\\n📝 {title}\")\n",
    "    display(Markdown(f\"```sql\\n{query}\\n```\"))\n",
    "\n",
    "def time_query(query, conn, title=\"Query\", show_results=True):\n",
    "    \"\"\"Execute query and measure time\"\"\"\n",
    "    start = time.time()\n",
    "    result = pd.read_sql(query, conn)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"⏱️ {title}: {elapsed:.4f} seconds\")\n",
    "    if show_results and len(result) > 0:\n",
    "        print(f\"📊 Returned {len(result):,} rows\")\n",
    "        if len(result) <= 5:\n",
    "            display(result)\n",
    "    return result, elapsed\n",
    "\n",
    "def explain_query(query, conn):\n",
    "    \"\"\"Show query execution plan\"\"\"\n",
    "    explain_query = f\"EXPLAIN QUERY PLAN {query}\"\n",
    "    plan = pd.read_sql(explain_query, conn)\n",
    "    print(\"\\n🔍 Query Execution Plan:\")\n",
    "    for _, row in plan.iterrows():\n",
    "        print(f\"  {row['detail']}\")\n",
    "    return plan\n",
    "\n",
    "print(\"✅ Environment ready for production SQL optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Setting Up Production-Scale Database\n",
    "\n",
    "We'll create a database with millions of rows to simulate production challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create production database\n",
    "conn = sqlite3.connect('production.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Enable query statistics\n",
    "cursor.execute(\"PRAGMA query_only = OFF\")\n",
    "cursor.execute(\"PRAGMA journal_mode = WAL\")  # Write-Ahead Logging for better concurrency\n",
    "\n",
    "# Drop existing tables\n",
    "tables = ['orders', 'order_items', 'customers', 'products', 'events']\n",
    "for table in tables:\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {table}\")\n",
    "    cursor.execute(f\"DROP INDEX IF EXISTS idx_{table}_all\")\n",
    "\n",
    "# Create tables WITHOUT indexes first (we'll add them strategically)\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE customers (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    email TEXT,\n",
    "    first_name TEXT,\n",
    "    last_name TEXT,\n",
    "    country TEXT,\n",
    "    city TEXT,\n",
    "    created_at TIMESTAMP,\n",
    "    lifetime_value DECIMAL(10,2),\n",
    "    segment TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE products (\n",
    "    product_id INTEGER PRIMARY KEY,\n",
    "    sku TEXT,\n",
    "    name TEXT,\n",
    "    category TEXT,\n",
    "    subcategory TEXT,\n",
    "    price DECIMAL(10,2),\n",
    "    cost DECIMAL(10,2),\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE orders (\n",
    "    order_id INTEGER PRIMARY KEY,\n",
    "    customer_id INTEGER,\n",
    "    order_date TIMESTAMP,\n",
    "    status TEXT,\n",
    "    total_amount DECIMAL(10,2),\n",
    "    shipping_country TEXT,\n",
    "    payment_method TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE order_items (\n",
    "    order_item_id INTEGER PRIMARY KEY,\n",
    "    order_id INTEGER,\n",
    "    product_id INTEGER,\n",
    "    quantity INTEGER,\n",
    "    unit_price DECIMAL(10,2),\n",
    "    discount DECIMAL(5,2)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE events (\n",
    "    event_id INTEGER PRIMARY KEY,\n",
    "    customer_id INTEGER,\n",
    "    event_type TEXT,\n",
    "    event_timestamp TIMESTAMP,\n",
    "    page_url TEXT,\n",
    "    session_id TEXT,\n",
    "    device_type TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"✅ Production schema created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large-scale data\n",
    "print(\"🔄 Generating production-scale data...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Scale parameters\n",
    "n_customers = 50000\n",
    "n_products = 5000\n",
    "n_orders = 200000\n",
    "n_events = 500000\n",
    "\n",
    "# Generate customers\n",
    "print(f\"  Creating {n_customers:,} customers...\")\n",
    "countries = ['USA', 'UK', 'Germany', 'France', 'Japan', 'Canada', 'Australia']\n",
    "segments = ['Premium', 'Standard', 'Basic', 'Inactive']\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'email': [f'user{i}@company.com' for i in range(1, n_customers + 1)],\n",
    "    'first_name': [f'First{i}' for i in range(1, n_customers + 1)],\n",
    "    'last_name': [f'Last{i}' for i in range(1, n_customers + 1)],\n",
    "    'country': np.random.choice(countries, n_customers),\n",
    "    'city': [f'City{i%100}' for i in range(n_customers)],\n",
    "    'created_at': pd.date_range('2020-01-01', periods=n_customers, freq='30s'),\n",
    "    'lifetime_value': np.random.exponential(500, n_customers),\n",
    "    'segment': np.random.choice(segments, n_customers, p=[0.1, 0.4, 0.4, 0.1])\n",
    "})\n",
    "\n",
    "# Generate products\n",
    "print(f\"  Creating {n_products:,} products...\")\n",
    "categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports', 'Toys']\n",
    "subcategories = ['Sub1', 'Sub2', 'Sub3', 'Sub4', 'Sub5']\n",
    "\n",
    "products = pd.DataFrame({\n",
    "    'product_id': range(1, n_products + 1),\n",
    "    'sku': [f'SKU{i:05d}' for i in range(1, n_products + 1)],\n",
    "    'name': [f'Product {i}' for i in range(1, n_products + 1)],\n",
    "    'category': np.random.choice(categories, n_products),\n",
    "    'subcategory': np.random.choice(subcategories, n_products),\n",
    "    'price': np.random.uniform(10, 1000, n_products),\n",
    "    'cost': np.random.uniform(5, 500, n_products),\n",
    "    'created_at': pd.date_range('2019-01-01', periods=n_products, freq='2H')\n",
    "})\n",
    "\n",
    "# Generate orders\n",
    "print(f\"  Creating {n_orders:,} orders...\")\n",
    "order_dates = pd.date_range('2022-01-01', '2024-12-31', freq='10s')[:n_orders]\n",
    "statuses = ['completed', 'pending', 'cancelled', 'refunded']\n",
    "payment_methods = ['credit_card', 'paypal', 'stripe', 'bank_transfer']\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': range(1, n_orders + 1),\n",
    "    'customer_id': np.random.randint(1, n_customers + 1, n_orders),\n",
    "    'order_date': order_dates,\n",
    "    'status': np.random.choice(statuses, n_orders, p=[0.7, 0.15, 0.1, 0.05]),\n",
    "    'total_amount': np.random.uniform(50, 2000, n_orders),\n",
    "    'shipping_country': np.random.choice(countries, n_orders),\n",
    "    'payment_method': np.random.choice(payment_methods, n_orders)\n",
    "})\n",
    "\n",
    "# Generate order items (2-5 items per order)\n",
    "print(f\"  Creating order items...\")\n",
    "order_items = []\n",
    "order_item_id = 1\n",
    "for order_id in range(1, min(n_orders + 1, 50000)):  # Limit for performance\n",
    "    n_items = np.random.randint(1, 6)\n",
    "    for _ in range(n_items):\n",
    "        order_items.append({\n",
    "            'order_item_id': order_item_id,\n",
    "            'order_id': order_id,\n",
    "            'product_id': np.random.randint(1, n_products + 1),\n",
    "            'quantity': np.random.randint(1, 5),\n",
    "            'unit_price': np.random.uniform(10, 500),\n",
    "            'discount': np.random.choice([0, 0.05, 0.10, 0.15, 0.20], p=[0.5, 0.2, 0.15, 0.1, 0.05])\n",
    "        })\n",
    "        order_item_id += 1\n",
    "\n",
    "order_items_df = pd.DataFrame(order_items)\n",
    "\n",
    "# Generate events\n",
    "print(f\"  Creating {n_events:,} events...\")\n",
    "event_types = ['page_view', 'add_to_cart', 'checkout', 'purchase', 'search']\n",
    "devices = ['mobile', 'desktop', 'tablet']\n",
    "\n",
    "events = pd.DataFrame({\n",
    "    'event_id': range(1, n_events + 1),\n",
    "    'customer_id': np.random.randint(1, n_customers + 1, n_events),\n",
    "    'event_type': np.random.choice(event_types, n_events, p=[0.5, 0.2, 0.1, 0.1, 0.1]),\n",
    "    'event_timestamp': pd.date_range('2024-01-01', periods=n_events, freq='5s'),\n",
    "    'page_url': [f'/page/{i%1000}' for i in range(n_events)],\n",
    "    'session_id': [f'session_{i//10}' for i in range(n_events)],\n",
    "    'device_type': np.random.choice(devices, n_events, p=[0.5, 0.4, 0.1])\n",
    "})\n",
    "\n",
    "# Load data into database\n",
    "print(\"\\n📥 Loading data into database...\")\n",
    "customers.to_sql('customers', conn, if_exists='append', index=False)\n",
    "products.to_sql('products', conn, if_exists='append', index=False)\n",
    "orders.to_sql('orders', conn, if_exists='append', index=False)\n",
    "order_items_df.to_sql('order_items', conn, if_exists='append', index=False)\n",
    "events.to_sql('events', conn, if_exists='append', index=False)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(f\"\\n✅ Production database populated:\")\n",
    "print(f\"  - {n_customers:,} customers\")\n",
    "print(f\"  - {n_products:,} products\")\n",
    "print(f\"  - {n_orders:,} orders\")\n",
    "print(f\"  - {len(order_items_df):,} order items\")\n",
    "print(f\"  - {n_events:,} events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔍 Part 1: Understanding Query Execution Plans\n",
    "\n",
    "The execution plan is your roadmap to understanding how the database processes queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Analyzing a Slow Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical dashboard query - SLOW VERSION\n",
    "slow_query = \"\"\"\n",
    "SELECT \n",
    "    c.country,\n",
    "    COUNT(DISTINCT o.customer_id) as unique_customers,\n",
    "    COUNT(o.order_id) as total_orders,\n",
    "    SUM(o.total_amount) as revenue\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "WHERE o.order_date >= '2024-01-01'\n",
    "  AND o.status = 'completed'\n",
    "  AND c.segment = 'Premium'\n",
    "GROUP BY c.country\n",
    "ORDER BY revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"🐌 SLOW QUERY - No Indexes\")\n",
    "show_sql(slow_query)\n",
    "\n",
    "# Explain the query\n",
    "explain_query(slow_query, conn)\n",
    "\n",
    "# Time the query\n",
    "result, slow_time = time_query(slow_query, conn, \"Slow Query\")\n",
    "\n",
    "print(\"\\n⚠️ Notice: SCAN TABLE means full table scan - very slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating Strategic Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add strategic indexes based on query patterns\n",
    "print(\"🔨 Creating strategic indexes...\\n\")\n",
    "\n",
    "index_queries = [\n",
    "    # Foreign key indexes\n",
    "    (\"CREATE INDEX idx_orders_customer_id ON orders(customer_id)\",\n",
    "     \"Foreign key index for JOIN\"),\n",
    "    \n",
    "    # Filter condition indexes\n",
    "    (\"CREATE INDEX idx_orders_date_status ON orders(order_date, status)\",\n",
    "     \"Composite index for WHERE clause\"),\n",
    "    \n",
    "    (\"CREATE INDEX idx_customers_segment ON customers(segment)\",\n",
    "     \"Index for segment filtering\"),\n",
    "    \n",
    "    # Covering index (includes all needed columns)\n",
    "    (\"CREATE INDEX idx_orders_covering ON orders(customer_id, order_date, status, total_amount)\",\n",
    "     \"Covering index - all data in index\")\n",
    "]\n",
    "\n",
    "for query, description in index_queries:\n",
    "    print(f\"📌 {description}\")\n",
    "    cursor.execute(f\"DROP INDEX IF EXISTS {query.split()[2]}\")\n",
    "    cursor.execute(query)\n",
    "    \n",
    "conn.commit()\n",
    "print(\"\\n✅ Indexes created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same query with indexes\n",
    "print(\"🚀 FAST QUERY - With Indexes\")\n",
    "\n",
    "# Explain the optimized query\n",
    "explain_query(slow_query, conn)\n",
    "\n",
    "# Time the query\n",
    "result, fast_time = time_query(slow_query, conn, \"Fast Query\")\n",
    "\n",
    "# Performance improvement\n",
    "if slow_time > 0:\n",
    "    improvement = (slow_time - fast_time) / slow_time * 100\n",
    "    speedup = slow_time / fast_time\n",
    "    print(f\"\\n🎉 Performance Improvement:\")\n",
    "    print(f\"  - {improvement:.1f}% faster\")\n",
    "    print(f\"  - {speedup:.1f}x speedup\")\n",
    "    print(f\"  - Saved {slow_time - fast_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📈 Part 2: Index Strategies\n",
    "\n",
    "Indexes are like a book's table of contents - they help find data quickly but take space to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Types of Indexes and When to Use Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different index types\n",
    "print(\"📚 INDEX TYPES AND USE CASES\\n\")\n",
    "\n",
    "index_examples = {\n",
    "    \"Single Column Index\": {\n",
    "        \"sql\": \"CREATE INDEX idx_customer_email ON customers(email)\",\n",
    "        \"use_case\": \"Lookups by email (WHERE email = 'user@example.com')\",\n",
    "        \"pros\": \"Fast for single column filters\",\n",
    "        \"cons\": \"Not helpful for multi-column filters\"\n",
    "    },\n",
    "    \"Composite Index\": {\n",
    "        \"sql\": \"CREATE INDEX idx_orders_composite ON orders(customer_id, order_date, status)\",\n",
    "        \"use_case\": \"Queries filtering on multiple columns\",\n",
    "        \"pros\": \"Covers multiple filter conditions\",\n",
    "        \"cons\": \"Column order matters! Only helps if leftmost columns are used\"\n",
    "    },\n",
    "    \"Covering Index\": {\n",
    "        \"sql\": \"CREATE INDEX idx_covering ON orders(customer_id, order_date) INCLUDE (total_amount)\",\n",
    "        \"use_case\": \"Query can be satisfied entirely from index\",\n",
    "        \"pros\": \"No table lookup needed - very fast\",\n",
    "        \"cons\": \"Larger index size\"\n",
    "    },\n",
    "    \"Partial Index\": {\n",
    "        \"sql\": \"CREATE INDEX idx_active_orders ON orders(order_date) WHERE status = 'completed'\",\n",
    "        \"use_case\": \"Frequently filter on specific value\",\n",
    "        \"pros\": \"Smaller index, faster for specific queries\",\n",
    "        \"cons\": \"Only helps for the specific condition\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for index_type, details in index_examples.items():\n",
    "    print(f\"📌 {index_type}\")\n",
    "    print(f\"   SQL: {details['sql']}\")\n",
    "    print(f\"   Use: {details['use_case']}\")\n",
    "    print(f\"   ✅ {details['pros']}\")\n",
    "    print(f\"   ⚠️ {details['cons']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Index Selectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze index selectivity\n",
    "selectivity_query = \"\"\"\n",
    "SELECT \n",
    "    'status' as column_name,\n",
    "    COUNT(DISTINCT status) as distinct_values,\n",
    "    COUNT(*) as total_rows,\n",
    "    ROUND(COUNT(DISTINCT status) * 100.0 / COUNT(*), 4) as selectivity_pct\n",
    "FROM orders\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'customer_id',\n",
    "    COUNT(DISTINCT customer_id),\n",
    "    COUNT(*),\n",
    "    ROUND(COUNT(DISTINCT customer_id) * 100.0 / COUNT(*), 4)\n",
    "FROM orders\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'payment_method',\n",
    "    COUNT(DISTINCT payment_method),\n",
    "    COUNT(*),\n",
    "    ROUND(COUNT(DISTINCT payment_method) * 100.0 / COUNT(*), 4)\n",
    "FROM orders\n",
    "ORDER BY selectivity_pct DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"📊 INDEX SELECTIVITY ANALYSIS\")\n",
    "print(\"Higher selectivity = Better index candidate\\n\")\n",
    "\n",
    "selectivity_result = pd.read_sql(selectivity_query, conn)\n",
    "display(selectivity_result)\n",
    "\n",
    "print(\"\\n💡 Insights:\")\n",
    "print(\"- High selectivity (>10%): Excellent for indexing\")\n",
    "print(\"- Medium selectivity (1-10%): Good for composite indexes\")\n",
    "print(\"- Low selectivity (<1%): Poor for single column index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Index Impact on Write Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure write performance with and without indexes\n",
    "print(\"⚖️ INDEX IMPACT ON WRITE OPERATIONS\\n\")\n",
    "\n",
    "# Test data for inserts\n",
    "test_orders = pd.DataFrame({\n",
    "    'order_id': range(1000000, 1001000),\n",
    "    'customer_id': np.random.randint(1, 50000, 1000),\n",
    "    'order_date': pd.date_range('2024-01-01', periods=1000, freq='1H'),\n",
    "    'status': np.random.choice(['completed', 'pending'], 1000),\n",
    "    'total_amount': np.random.uniform(50, 500, 1000),\n",
    "    'shipping_country': np.random.choice(['USA', 'UK'], 1000),\n",
    "    'payment_method': np.random.choice(['credit_card', 'paypal'], 1000)\n",
    "})\n",
    "\n",
    "# Test with indexes\n",
    "print(\"With indexes:\")\n",
    "start = time.time()\n",
    "test_orders.head(100).to_sql('orders', conn, if_exists='append', index=False)\n",
    "with_index_time = time.time() - start\n",
    "print(f\"  Insert time: {with_index_time:.4f} seconds\")\n",
    "\n",
    "# Drop indexes\n",
    "cursor.execute(\"DROP INDEX IF EXISTS idx_orders_customer_id\")\n",
    "cursor.execute(\"DROP INDEX IF EXISTS idx_orders_date_status\")\n",
    "\n",
    "# Test without indexes\n",
    "print(\"\\nWithout indexes:\")\n",
    "start = time.time()\n",
    "test_orders.iloc[100:200].to_sql('orders', conn, if_exists='append', index=False)\n",
    "without_index_time = time.time() - start\n",
    "print(f\"  Insert time: {without_index_time:.4f} seconds\")\n",
    "\n",
    "# Restore indexes\n",
    "cursor.execute(\"CREATE INDEX idx_orders_customer_id ON orders(customer_id)\")\n",
    "cursor.execute(\"CREATE INDEX idx_orders_date_status ON orders(order_date, status)\")\n",
    "\n",
    "print(f\"\\n📊 Write Performance Impact:\")\n",
    "print(f\"  Indexes add {((with_index_time - without_index_time) / without_index_time * 100):.1f}% overhead to inserts\")\n",
    "print(\"\\n💡 Trade-off: Slower writes for much faster reads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 Part 3: Query Optimization Techniques\n",
    "\n",
    "Beyond indexes, there are many ways to optimize queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimizing JOIN Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different JOIN strategies\n",
    "print(\"🔗 JOIN OPTIMIZATION STRATEGIES\\n\")\n",
    "\n",
    "# Inefficient: JOIN then filter\n",
    "inefficient_join = \"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.order_date >= '2024-01-01'\n",
    "  AND p.category = 'Electronics'\n",
    "\"\"\"\n",
    "\n",
    "# Efficient: Filter before JOIN\n",
    "efficient_join = \"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id \n",
    "    FROM orders \n",
    "    WHERE order_date >= '2024-01-01'\n",
    ") o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN (\n",
    "    SELECT product_id \n",
    "    FROM products \n",
    "    WHERE category = 'Electronics'\n",
    ") p ON oi.product_id = p.product_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"❌ INEFFICIENT: Join all, then filter\")\n",
    "show_sql(inefficient_join)\n",
    "_, inefficient_time = time_query(inefficient_join, conn, \"Inefficient JOIN\", show_results=False)\n",
    "\n",
    "print(\"\\n✅ EFFICIENT: Filter first, then join\")\n",
    "show_sql(efficient_join)\n",
    "_, efficient_time = time_query(efficient_join, conn, \"Efficient JOIN\", show_results=False)\n",
    "\n",
    "if inefficient_time > 0:\n",
    "    print(f\"\\n🎯 Improvement: {(inefficient_time / efficient_time):.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Avoiding N+1 Query Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate N+1 problem and solution\n",
    "print(\"🔄 N+1 QUERY PROBLEM\\n\")\n",
    "\n",
    "# N+1 Problem: Separate query for each customer\n",
    "print(\"❌ N+1 PROBLEM: Multiple queries\")\n",
    "start = time.time()\n",
    "\n",
    "# Get customers\n",
    "customers_sample = pd.read_sql(\n",
    "    \"SELECT customer_id FROM customers WHERE segment = 'Premium' LIMIT 10\", \n",
    "    conn\n",
    ")\n",
    "\n",
    "# For each customer, get their orders (N+1 queries)\n",
    "results = []\n",
    "for customer_id in customers_sample['customer_id']:\n",
    "    orders = pd.read_sql(\n",
    "        f\"SELECT COUNT(*) as order_count FROM orders WHERE customer_id = {customer_id}\",\n",
    "        conn\n",
    "    )\n",
    "    results.append(orders)\n",
    "\n",
    "n_plus_one_time = time.time() - start\n",
    "print(f\"  Time: {n_plus_one_time:.4f} seconds\")\n",
    "print(f\"  Queries executed: {len(customers_sample) + 1}\")\n",
    "\n",
    "# Solution: Single query with JOIN\n",
    "print(\"\\n✅ SOLUTION: Single query with JOIN\")\n",
    "single_query = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    COUNT(o.order_id) as order_count\n",
    "FROM customers c\n",
    "LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE c.segment = 'Premium'\n",
    "GROUP BY c.customer_id\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "_, single_time = time_query(single_query, conn, \"Single Query\", show_results=False)\n",
    "print(f\"  Queries executed: 1\")\n",
    "\n",
    "if n_plus_one_time > 0:\n",
    "    print(f\"\\n🎯 Improvement: {(n_plus_one_time / single_time):.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Query Result Caching Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement materialized view pattern\n",
    "print(\"💾 MATERIALIZED VIEW PATTERN\\n\")\n",
    "\n",
    "# Create a materialized view (summary table)\n",
    "print(\"Creating materialized summary...\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS daily_sales_summary\")\n",
    "\n",
    "create_summary = \"\"\"\n",
    "CREATE TABLE daily_sales_summary AS\n",
    "SELECT \n",
    "    DATE(order_date) as sale_date,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(order_id) as order_count,\n",
    "    SUM(total_amount) as revenue,\n",
    "    AVG(total_amount) as avg_order_value\n",
    "FROM orders\n",
    "WHERE status = 'completed'\n",
    "GROUP BY DATE(order_date)\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_summary)\n",
    "cursor.execute(\"CREATE INDEX idx_summary_date ON daily_sales_summary(sale_date)\")\n",
    "conn.commit()\n",
    "\n",
    "# Compare query performance\n",
    "print(\"\\n📊 Performance Comparison:\\n\")\n",
    "\n",
    "# Direct query\n",
    "direct_query = \"\"\"\n",
    "SELECT \n",
    "    DATE(order_date) as sale_date,\n",
    "    SUM(total_amount) as revenue\n",
    "FROM orders\n",
    "WHERE status = 'completed'\n",
    "  AND order_date >= '2024-01-01'\n",
    "  AND order_date <= '2024-01-31'\n",
    "GROUP BY DATE(order_date)\n",
    "\"\"\"\n",
    "\n",
    "print(\"❌ Direct aggregation:\")\n",
    "_, direct_time = time_query(direct_query, conn, \"Direct Query\", show_results=False)\n",
    "\n",
    "# Using materialized view\n",
    "summary_query = \"\"\"\n",
    "SELECT \n",
    "    sale_date,\n",
    "    revenue\n",
    "FROM daily_sales_summary\n",
    "WHERE sale_date >= '2024-01-01'\n",
    "  AND sale_date <= '2024-01-31'\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n✅ Using materialized view:\")\n",
    "_, summary_time = time_query(summary_query, conn, \"Summary Query\", show_results=False)\n",
    "\n",
    "if direct_time > 0:\n",
    "    print(f\"\\n🎯 Improvement: {(direct_time / summary_time):.1f}x faster\")\n",
    "    print(\"\\n💡 Trade-off: Requires periodic refresh of summary table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🏭 Part 4: Production Best Practices\n",
    "\n",
    "Writing SQL for production requires different considerations than ad-hoc analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Query Monitoring and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query performance monitoring\n",
    "print(\"📊 QUERY PERFORMANCE MONITORING\\n\")\n",
    "\n",
    "# Create a query log table\n",
    "cursor.execute(\"DROP TABLE IF EXISTS query_log\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE query_log (\n",
    "    query_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    query_text TEXT,\n",
    "    execution_time DECIMAL(10,4),\n",
    "    rows_affected INTEGER,\n",
    "    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "def log_query(query, conn):\n",
    "    \"\"\"Execute query with logging\"\"\"\n",
    "    start = time.time()\n",
    "    result = pd.read_sql(query, conn)\n",
    "    execution_time = time.time() - start\n",
    "    \n",
    "    # Log the query\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO query_log (query_text, execution_time, rows_affected) VALUES (?, ?, ?)\",\n",
    "        (query[:100], execution_time, len(result))\n",
    "    )\n",
    "    conn.commit()\n",
    "    return result, execution_time\n",
    "\n",
    "# Run some test queries\n",
    "test_queries = [\n",
    "    \"SELECT COUNT(*) FROM orders\",\n",
    "    \"SELECT * FROM customers WHERE segment = 'Premium' LIMIT 10\",\n",
    "    \"SELECT category, COUNT(*) FROM products GROUP BY category\"\n",
    "]\n",
    "\n",
    "print(\"Running test queries...\")\n",
    "for query in test_queries:\n",
    "    _, exec_time = log_query(query, conn)\n",
    "    print(f\"  ✓ Query executed in {exec_time:.4f}s\")\n",
    "\n",
    "# Analyze query performance\n",
    "performance_report = \"\"\"\n",
    "SELECT \n",
    "    query_text,\n",
    "    execution_time,\n",
    "    rows_affected,\n",
    "    timestamp\n",
    "FROM query_log\n",
    "ORDER BY execution_time DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n📈 Query Performance Report:\")\n",
    "report = pd.read_sql(performance_report, conn)\n",
    "display(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Connection Pooling and Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate connection pooling pattern\n",
    "print(\"🔌 CONNECTION POOLING PATTERN\\n\")\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import queue\n",
    "\n",
    "class ConnectionPool:\n",
    "    \"\"\"Simple connection pool implementation\"\"\"\n",
    "    def __init__(self, database, max_connections=5):\n",
    "        self.database = database\n",
    "        self.max_connections = max_connections\n",
    "        self.pool = queue.Queue(maxsize=max_connections)\n",
    "        \n",
    "        # Initialize pool with connections\n",
    "        for _ in range(max_connections):\n",
    "            conn = sqlite3.connect(database)\n",
    "            self.pool.put(conn)\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get connection from pool\"\"\"\n",
    "        conn = self.pool.get()\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            # Return connection to pool\n",
    "            self.pool.put(conn)\n",
    "    \n",
    "    def close_all(self):\n",
    "        \"\"\"Close all connections\"\"\"\n",
    "        while not self.pool.empty():\n",
    "            conn = self.pool.get()\n",
    "            conn.close()\n",
    "\n",
    "# Example usage\n",
    "pool = ConnectionPool('production.db', max_connections=3)\n",
    "\n",
    "print(\"Using connection pool:\")\n",
    "for i in range(5):\n",
    "    with pool.get_connection() as conn:\n",
    "        result = pd.read_sql(\"SELECT COUNT(*) as cnt FROM orders\", conn)\n",
    "        print(f\"  Query {i+1}: {result['cnt'].iloc[0]:,} orders\")\n",
    "\n",
    "pool.close_all()\n",
    "print(\"\\n✅ Connection pool prevents connection exhaustion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SQL Injection Prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate SQL injection prevention\n",
    "print(\"🔒 SQL INJECTION PREVENTION\\n\")\n",
    "\n",
    "# Vulnerable code - DON'T DO THIS!\n",
    "def vulnerable_query(user_input):\n",
    "    # Direct string concatenation - DANGEROUS!\n",
    "    query = f\"SELECT * FROM customers WHERE email = '{user_input}'\"\n",
    "    return query\n",
    "\n",
    "# Safe code - DO THIS!\n",
    "def safe_query(user_input, conn):\n",
    "    # Parameterized query - SAFE!\n",
    "    query = \"SELECT * FROM customers WHERE email = ?\"\n",
    "    return pd.read_sql(query, conn, params=(user_input,))\n",
    "\n",
    "# Example malicious input\n",
    "malicious_input = \"admin@example.com' OR '1'='1\"\n",
    "\n",
    "print(\"❌ VULNERABLE Query:\")\n",
    "vulnerable = vulnerable_query(malicious_input)\n",
    "print(f\"  {vulnerable}\")\n",
    "print(\"  ⚠️ This would return ALL customers!\")\n",
    "\n",
    "print(\"\\n✅ SAFE Query:\")\n",
    "print(\"  Using parameterized query with ?\")\n",
    "print(\"  The malicious input is treated as a literal string\")\n",
    "\n",
    "# Best practices\n",
    "print(\"\\n📝 SQL Injection Prevention Rules:\")\n",
    "print(\"1. ALWAYS use parameterized queries (? or :param)\")\n",
    "print(\"2. NEVER concatenate user input into SQL strings\")\n",
    "print(\"3. Validate and sanitize all inputs\")\n",
    "print(\"4. Use stored procedures where appropriate\")\n",
    "print(\"5. Apply principle of least privilege to database users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 Part 5: Database Statistics and Maintenance\n",
    "\n",
    "Keep your database healthy with regular maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze database statistics\n",
    "print(\"📊 DATABASE STATISTICS\\n\")\n",
    "\n",
    "# Table sizes\n",
    "size_query = \"\"\"\n",
    "SELECT \n",
    "    name as table_name,\n",
    "    (SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND tbl_name=m.name) as index_count\n",
    "FROM sqlite_master m\n",
    "WHERE type = 'table'\n",
    "  AND name NOT LIKE 'sqlite_%'\n",
    "ORDER BY name\n",
    "\"\"\"\n",
    "\n",
    "table_stats = pd.read_sql(size_query, conn)\n",
    "\n",
    "# Add row counts\n",
    "for idx, row in table_stats.iterrows():\n",
    "    count_query = f\"SELECT COUNT(*) as cnt FROM {row['table_name']}\"\n",
    "    count = pd.read_sql(count_query, conn)['cnt'].iloc[0]\n",
    "    table_stats.at[idx, 'row_count'] = count\n",
    "\n",
    "print(\"Table Statistics:\")\n",
    "display(table_stats)\n",
    "\n",
    "# Database maintenance commands\n",
    "print(\"\\n🔧 MAINTENANCE OPERATIONS:\\n\")\n",
    "\n",
    "# Analyze tables for query optimizer\n",
    "print(\"1. Updating statistics...\")\n",
    "cursor.execute(\"ANALYZE\")\n",
    "print(\"   ✓ Statistics updated\")\n",
    "\n",
    "# Vacuum to reclaim space\n",
    "print(\"\\n2. Vacuuming database...\")\n",
    "cursor.execute(\"VACUUM\")\n",
    "print(\"   ✓ Space reclaimed\")\n",
    "\n",
    "# Check integrity\n",
    "print(\"\\n3. Checking integrity...\")\n",
    "integrity = cursor.execute(\"PRAGMA integrity_check\").fetchone()\n",
    "print(f\"   ✓ Integrity: {integrity[0]}\")\n",
    "\n",
    "print(\"\\n✅ Database maintenance complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Practice Exercises\n",
    "\n",
    "Apply what you've learned to optimize real queries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Optimize a Slow Dashboard Query\n",
    "\n",
    "This query powers a executive dashboard but takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow dashboard query\n",
    "dashboard_query = \"\"\"\n",
    "SELECT \n",
    "    c.segment,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT o.customer_id) as customers,\n",
    "    COUNT(DISTINCT o.order_id) as orders,\n",
    "    SUM(oi.quantity * oi.unit_price) as revenue\n",
    "FROM order_items oi\n",
    "JOIN orders o ON oi.order_id = o.order_id\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.order_date >= '2024-01-01'\n",
    "GROUP BY c.segment, p.category\n",
    "ORDER BY revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"TODO: Optimize this query\")\n",
    "print(\"Hints:\")\n",
    "print(\"1. Check what indexes exist\")\n",
    "print(\"2. Consider creating a summary table\")\n",
    "print(\"3. Think about filtering early\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Design Indexes for Common Queries\n",
    "\n",
    "Given these common query patterns, design optimal indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common query patterns\n",
    "queries = [\n",
    "    \"SELECT * FROM orders WHERE customer_id = ? AND status = 'completed'\",\n",
    "    \"SELECT * FROM products WHERE category = ? ORDER BY price DESC\",\n",
    "    \"SELECT * FROM events WHERE customer_id = ? AND event_timestamp >= ?\",\n",
    "    \"SELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id\"\n",
    "]\n",
    "\n",
    "print(\"TODO: Design indexes for these queries\")\n",
    "print(\"Consider:\")\n",
    "print(\"- Single vs composite indexes\")\n",
    "print(\"- Column order in composite indexes\")\n",
    "print(\"- Covering indexes where beneficial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎓 Key Takeaways\n",
    "\n",
    "1. **Query Plans Are Your Friend**:\n",
    "   - Always EXPLAIN before optimizing\n",
    "   - Look for full table scans (SCAN TABLE)\n",
    "   - Prefer index seeks over scans\n",
    "\n",
    "2. **Index Strategy**:\n",
    "   - Index foreign keys and WHERE clause columns\n",
    "   - Consider composite indexes for multi-column filters\n",
    "   - Remember: indexes slow down writes\n",
    "\n",
    "3. **Query Optimization**:\n",
    "   - Filter early, join late\n",
    "   - Avoid N+1 queries\n",
    "   - Use materialized views for complex aggregations\n",
    "\n",
    "4. **Production Best Practices**:\n",
    "   - Always use parameterized queries\n",
    "   - Implement connection pooling\n",
    "   - Monitor and log slow queries\n",
    "   - Regular maintenance (ANALYZE, VACUUM)\n",
    "\n",
    "5. **Performance Metrics**:\n",
    "   - Measure before and after optimization\n",
    "   - Consider both read and write performance\n",
    "   - Profile in production-like environment\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "You're now ready to:\n",
    "- Optimize slow queries in production\n",
    "- Design efficient database schemas\n",
    "- Implement monitoring and maintenance\n",
    "- Scale to millions of rows\n",
    "\n",
    "Remember: **Performance is a feature!** ⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "conn.close()\n",
    "print(\"✅ Database connection closed. Great work on production SQL optimization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
